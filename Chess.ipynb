{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4XQAZf8zoZQW0YBdkQTWw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xelaro2304/MSB1015-Scientific-Programming/blob/main/Chess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ydata-profiling\n",
        "!pip install berserk\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "d-16AgAau3KU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7_5vMPhC1vZ"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os\n",
        "import berserk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from ydata_profiling import ProfileReport\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_distribution(data, plot=\"hist\", title=None, label=None, bins=30, show_stats=True, normalize=False):\n",
        "    \"\"\"\n",
        "    Plot a histogram (numeric) or count plot (categorical) for a single variable.\n",
        "\n",
        "    Parameters:\n",
        "    - data: array-like, the variable to plot\n",
        "    - plot: \"hist\" for histogram, \"count\" for categorical count plot\n",
        "    - title: optional plot title\n",
        "    - label: optional x-axis label\n",
        "    - bins: number of bins for histogram\n",
        "    - show_stats: show mean/median/mode (only for histogram)\n",
        "    - normalize: bool, whether to normalize frequencies/counts (0-1 or percentages)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(6,6))\n",
        "\n",
        "    if plot == \"hist\":\n",
        "        stat_type = 'density' if normalize else 'count'\n",
        "        sns.histplot(data, bins=bins, kde=False, color=sns.color_palette(\"colorblind\")[0], stat=stat_type)\n",
        "\n",
        "        if show_stats:\n",
        "            mean_val = np.mean(data)\n",
        "            median_val = np.median(data)\n",
        "            mode_val = stats.mode(data, keepdims=True)[0][0]\n",
        "            plt.axvline(mean_val, color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Mean = {mean_val:.2f}\")\n",
        "            plt.axvline(median_val, color=\"green\", linestyle=\"--\", linewidth=1.5, label=f\"Median = {median_val:.2f}\")\n",
        "            plt.axvline(mode_val, color=\"blue\", linestyle=\"--\", linewidth=1.5, label=f\"Mode = {mode_val:.2f}\")\n",
        "            plt.legend()\n",
        "\n",
        "        plt.ylabel(\"Density\" if normalize else \"Frequency\")\n",
        "        plt.xlabel(label if label else \"Value\")\n",
        "\n",
        "    elif plot == \"count\":\n",
        "        counts = data.value_counts(normalize=normalize)\n",
        "        counts.plot(kind='bar', color=sns.color_palette(\"colorblind\", len(counts)))\n",
        "        plt.ylabel(\"Proportion\" if normalize else \"Count\")\n",
        "        plt.xlabel(label if label else \"Category\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"plot must be either 'hist' or 'count'\")\n",
        "\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_winner_by(df, col, title=None, ylabel=None):\n",
        "    \"\"\"\n",
        "    Plot horizontal percentage-stacked bar chart for chess game results.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing 'winner' and the numeric column.\n",
        "    - col: Column name to group by (e.g., 'start_time', 'increment', 'avg_rating').\n",
        "    - title: Plot title.\n",
        "    - ylabel: Label for y-axis.\n",
        "    \"\"\"\n",
        "    df_plot = df.copy()\n",
        "\n",
        "    if col == 'avg_rating':\n",
        "        bins = [-np.inf, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000,\n",
        "                2100, 2200, 2300, 2400, np.inf]\n",
        "        labels = [\"< 1000\",\"1000-1100\",\"1100-1200\",\"1200-1300\",\"1300-1400\",\"1400-1500\",\n",
        "                  \"1500-1600\",\"1600-1700\",\"1700-1800\",\"1800-1900\",\"1900-2000\",\"2000-2100\",\n",
        "                  \"2100-2200\",\"2200-2300\",\"2300-2400\",\"> 2400\"]\n",
        "        df_plot[col] = pd.cut(df_plot[col], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "    if col == 'rating_diff':\n",
        "        df_plot['rating_diff_c2'] = np.where(df_plot['rating_diff'] > 0,\n",
        "                                            \"White higher rating\", \"White not-higher rating\")\n",
        "        # Count per category and winner\n",
        "        count_df = df_plot.groupby(['rating_diff_c2', 'winner']).size().reset_index(name='count')\n",
        "        count_pivot = count_df.pivot(index='rating_diff_c2', columns='winner', values='count').fillna(0)\n",
        "\n",
        "        # Plot heatmap\n",
        "        plt.figure(figsize=(8,6))\n",
        "        sns.heatmap(count_pivot, annot=True, fmt='g', cmap='Greys', linewidths=0.8, linecolor='black', cbar=False,\n",
        "                    annot_kws={\"size\": 15})\n",
        "        plt.title(title if title else \"Result of games by categorical difference in rating\")\n",
        "        plt.xlabel(\"Colour of winner\")\n",
        "        plt.ylabel(\"Rating before game\")\n",
        "        plt.show()\n",
        "        return\n",
        "\n",
        "    # --- Count per category and winner ---\n",
        "    count_df = df_plot.groupby([col, 'winner']).size().reset_index(name='count')\n",
        "\n",
        "    # --- Pivot for stacked percentage plot ---\n",
        "    count_pivot = count_df.pivot(index=col, columns='winner', values='count').fillna(0)\n",
        "    count_pct = count_pivot.div(count_pivot.sum(axis=1), axis=0)\n",
        "    count_pct = count_pct[['white', 'draw', 'black']]  # desired stacking order\n",
        "\n",
        "    # --- Define colors ---\n",
        "    color_map = {'white': '#d9d9d9', 'draw': 'grey', 'black': 'black'}\n",
        "\n",
        "    # --- Sort descending by the column ---\n",
        "    count_pct = count_pct.sort_index(ascending=False)\n",
        "    count_pivot = count_pivot.loc[count_pct.index]\n",
        "\n",
        "    # --- Plot ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    count_pct.plot(kind='barh', stacked=True,\n",
        "                   color=[color_map.get(c, 'grey') for c in count_pct.columns],\n",
        "                   alpha=0.95, width=1, edgecolor='black', ax=ax)\n",
        "\n",
        "    ax.set_xlabel(\"Share of wins\")\n",
        "    ax.set_ylabel(ylabel if ylabel else col)\n",
        "    ax.set_title(title if title else f\"Result of games by {col}\")\n",
        "    ax.legend(title=\"Colour of winner\")\n",
        "    ax.xaxis.set_major_formatter(plt.matplotlib.ticker.PercentFormatter(1.0))\n",
        "\n",
        "    # --- Add counts in the middle of each segment ---\n",
        "    for i, val in enumerate(count_pct.index):\n",
        "        left = 0\n",
        "        for winner in count_pct.columns:\n",
        "            frac = count_pct.loc[val, winner]  # fraction for plotting\n",
        "            value = count_pivot.loc[val, winner]  # raw count\n",
        "            if value > 0:\n",
        "                text_color = 'black' if winner == 'white' else 'white'\n",
        "                ax.text(left + frac/2, i, int(value), ha='center', va='center',\n",
        "                        color=text_color, fontsize=12)\n",
        "                left += frac\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "MSUO8GpUMUb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://docs.google.com/uc?export=download&id=1lBXYMdZtKdMm4AtGWjJFjmBygUtn8w5y&confirm=t'\n",
        "path = os.getcwd()\n",
        "output = path + '/games.csv'\n",
        "!wget -O $output 'https://docs.google.com/uc?export=download&id=1lBXYMdZtKdMm4AtGWjJFjmBygUtn8w5y&confirm=t'"
      ],
      "metadata": {
        "id": "CBYR4gK9pfsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_df = pd.read_csv(output, sep=';')\n"
      ],
      "metadata": {
        "id": "mnpv2wWym0Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_df.head()"
      ],
      "metadata": {
        "id": "m4oBpkaYrGgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_df.shape"
      ],
      "metadata": {
        "id": "16XBWDVzrHco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_df.info()"
      ],
      "metadata": {
        "id": "0jySP37brgOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_df.isnull().sum()"
      ],
      "metadata": {
        "id": "JAejUC_UrqlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_df.describe()"
      ],
      "metadata": {
        "id": "1LTJhQ2ZrynZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything seems normal except for that minimum white rating, which will be inspected further"
      ],
      "metadata": {
        "id": "IkCoEP1DuE4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_rating = games_df[\"white_rating\"]\n",
        "negative_rating = negative_rating[negative_rating < 0]\n",
        "print('Number of negative values:', len(negative_rating))\n",
        "negative_rating.head()"
      ],
      "metadata": {
        "id": "erp2GE1W3HXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is another game with a negative value for a rating"
      ],
      "metadata": {
        "id": "MKfqNWvT5Gh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_rating_indices = list(negative_rating.index)\n",
        "negative_rating_info = games_df.iloc[list(negative_rating_indices),]\n",
        "negative_rating_info.head()"
      ],
      "metadata": {
        "id": "d1cIctKK3N9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will check the original values of the game by fetching it with game ID"
      ],
      "metadata": {
        "id": "5DbSoeK_7RpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./token') as f:\n",
        "\n",
        "    token = f.read()\n",
        "    token = token.strip()\n",
        "\n",
        "\n",
        "session = berserk.TokenSession(token)\n",
        "\n",
        "client = berserk.Client(session)"
      ],
      "metadata": {
        "id": "_8fBU10b7XGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_rating_games = list(negative_rating_info[\"id\"])\n",
        "corrected_ratings = []\n",
        "for g in negative_rating_games:\n",
        "    game = client.games.export(g, as_pgn=True)\n",
        "    print(game)\n",
        "    game = game.split('\\n')\n",
        "    corrected_ratings.append(int(game[9][11:15]))\n",
        "print(corrected_ratings)"
      ],
      "metadata": {
        "id": "WALu11H1-Leu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_positive_rtg = games_df\n",
        "games_positive_rtg.loc[negative_rating_indices, 'white_rating'] = corrected_ratings\n",
        "games_positive_rtg.loc[negative_rating_indices]"
      ],
      "metadata": {
        "id": "E1vBf-sbMBYX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(games_df,title=\"Games report\")\n",
        "\n",
        "profile.to_file(\"games_report.html\")\n"
      ],
      "metadata": {
        "id": "B9wHgeX7usKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!env BROWSER=firefox\n",
        "#!open games_report.html\n",
        "from IPython.display import HTML\n",
        "\n",
        "# show an HTML file inside the notebook\n",
        "HTML(filename=\"games_report.html\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RUDJgnUhwuqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hexbin(games_df['white_rating'], games_df['black_rating'], gridsize=20, cmap='viridis')\n",
        "plt.colorbar(label=\"Number of games\")\n",
        "plt.xlabel(\"White rating\")\n",
        "plt.ylabel(\"Black rating\")\n",
        "plt.title(\"Player ratings heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RHHEwJ5Zc6n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average rating\n",
        "avg_rating = games_df\n",
        "games_df['avg_rating'] = (games_df['white_rating'] + games_df['black_rating']) / 2\n",
        "plot_winner_by(games_df, 'avg_rating', title=\"Result of games by average rating\", ylabel=\"Average rating\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V8xezjJAYcjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_diff = games_df['white_rating'] - games_df['black_rating']\n",
        "sns.histplot(rating_diff, kde=True, bins=50)\n",
        "plt.xlabel(\"Rating difference (White - Black)\")\n",
        "plt.title(\"Distribution of rating differences\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FIPe9x74dH4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things to notice:\n",
        "\n",
        "1.   There seem to be some duplicated instances\n",
        "1.   There are 400 unique increment codes, which seems problematic to use for classification\n",
        "1.   Winner classes are somewhat balanced, except for the amount of draws\n",
        "2.   The number of draws in winner is higher than the number of draws in victory status, will need to check that\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vP_LZbkvhbTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duplicated instances"
      ],
      "metadata": {
        "id": "owp4u2GKv6_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "game_ids = games_positive_rtg['id']\n",
        "#duplicates = [i for i in game_ids if game_ids.count(i) > 1]\n",
        "#print(duplicates)\n",
        "print('Number of unique records:', len(games_positive_rtg['id'].unique()))\n",
        "duplicate_counts = games_positive_rtg['id'].value_counts()\n",
        "duplicate_ids = list(duplicate_counts[duplicate_counts > 1].index)\n",
        "duplicate_counts = duplicate_counts[duplicate_counts > 1]\n",
        "print('Total number of duplicated records:', sum(duplicate_counts))\n",
        "print('Number of records duplicated:', len(duplicate_counts))\n",
        "print('Duplicated ids:', duplicate_ids)\n",
        "\n",
        "plot_distribution(duplicate_counts.values, 'hist', 'Number of Duplicates per Game ID', 'Amount of times duplicated', show_stats=False)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HNEu2zn2pvEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of the 20,058 records, 19113 are unique, but it is detecting only 813 replicates instead of 945\n",
        "\n",
        "Repetition is mainly occuring in duplicates, although some of them are repeated 3-5 times"
      ],
      "metadata": {
        "id": "ckVZP3v6xAm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "games_unique = games_positive_rtg.drop_duplicates(keep='first')\n",
        "print(f\"Original rows: {len(games_positive_rtg)}, After removing duplicates: {len(games_unique)}\")"
      ],
      "metadata": {
        "id": "nHqH3AOnLlHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to remove duplicates values only removes ~400 of them, so I'll inspect further"
      ],
      "metadata": {
        "id": "REPNy9IoxK0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_sample = games_positive_rtg[games_positive_rtg['id'].isin(duplicate_ids[0:4])]\n",
        "duplicate_sample.sort_values(by='id')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uT7ZuCGpxvPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the repeated instances have distinct values of \"created_at\" and \"last_move_at\", so I'll try removing it"
      ],
      "metadata": {
        "id": "ikImkRV6z3Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "games_time_dropped = games_positive_rtg.drop(columns=['created_at', 'last_move_at'])\n",
        "#games_time_dropped = games_positive_rtg.drop('last_move_at', axis=1)\n",
        "\n",
        "games_unique = games_time_dropped.drop_duplicates(keep='first').reset_index(drop=True)\n",
        "print(f\"Original rows: {len(games_time_dropped)}, After removing duplicates: {len(games_unique)}\")"
      ],
      "metadata": {
        "id": "oLMrIWRL0QJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All duplicates removed"
      ],
      "metadata": {
        "id": "DPmcgL1z1-3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert increment codes"
      ],
      "metadata": {
        "id": "iPzDDodQR9CJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll try to handle the increment code in two ways:\n",
        "\n",
        "\n",
        "1.   Separate time into minutes and time increment per move\n",
        "2.   Classify each increment code into a time control\n",
        "\n"
      ],
      "metadata": {
        "id": "IXTlPx3LUImw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "increment_code = games_unique['increment_code']\n",
        "increment_code_split = [time.split('+') for time in increment_code]\n",
        "print('Splitted increment codes:', increment_code_split)\n",
        "\n",
        "#As minutes and increment\n",
        "start_time = [int(minutes[0]) for minutes in increment_code_split]\n",
        "print('Starting time in minutes:', start_time)\n",
        "\n",
        "#bar_chart(list(games_unique.iloc()), start_time, 'Starting time per game ID')\n",
        "\n",
        "increment = [int(seconds[1]) for seconds in increment_code_split]\n",
        "print('Increment in seconds:', increment)\n",
        "\n"
      ],
      "metadata": {
        "id": "NMOMe-v5P5y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_distribution(start_time, 'hist', 'Starting time per game ID', 'Minutes', show_stats=True)"
      ],
      "metadata": {
        "id": "ekjG5XXoMg_u",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_distribution(increment, 'hist', 'Increment per game ID', 'Seconds', show_stats=True)"
      ],
      "metadata": {
        "id": "xBDHJwavMn5I",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time_df = pd.DataFrame(start_time, columns=['start_time'])\n",
        "increment_df = pd.DataFrame(increment, columns=['increment'])\n",
        "#check if there are games with 0 < start time < 1\n",
        "#games_unique\n",
        "under_minute = ((start_time_df < 1) & (start_time_df > 0)).sum()\n",
        "print('Games with less than 1 minute of start time:', under_minute.iloc[0])\n"
      ],
      "metadata": {
        "id": "PTdeHlQuOpJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most games are finish (no increment) and have 10 minutes as start time, with no game starting with less than a minute"
      ],
      "metadata": {
        "id": "RaBIu2LLM9D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "games_unique = pd.concat([games_unique, start_time_df], axis = 1)\n",
        "games_unique = pd.concat([games_unique, increment_df], axis = 1)\n",
        "games_unique.info()"
      ],
      "metadata": {
        "id": "lEQ3Lu1eSiUs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_winner_by(games_unique, 'start_time', title=\"Result of games by start time\", ylabel=\"Start time (minutes)\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aRkFW_dxWMPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_winner_by(games_unique, 'increment', title=\"Result of games by increment\", ylabel=\"Increment (seconds)\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pMIQjcH0WNwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_unique.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HAexyrtDWX8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_unique.head()"
      ],
      "metadata": {
        "id": "NYKIaVrwaXNp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the data source, Lichess, time controls are decided assuming a game length of 40 moves and assigning the following categories depending on the duration:\n",
        "\n",
        "    ≤ 29s = UltraBullet\n",
        "    ≤ 179s = Bullet\n",
        "    ≤ 479s = Blitz\n",
        "    ≤ 1499s = Rapid\n",
        "    ≥ 1500s = Classical"
      ],
      "metadata": {
        "id": "X437Tun_NLgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_time_control(minutes, increment):\n",
        "    total_time = minutes*60+increment*40\n",
        "    if total_time <= 29:\n",
        "        return 'UltraBullet'\n",
        "    elif total_time <= 179:\n",
        "        return 'Bullet'\n",
        "    elif total_time <= 479:\n",
        "        return 'Blitz'\n",
        "    elif total_time <= 1499:\n",
        "        return 'Rapid'\n",
        "    else:\n",
        "        return 'Classical'\n",
        "\n",
        "time_control = games_unique.apply(lambda x: set_time_control(x['start_time'], x['increment']), axis=1)\n",
        "time_control_df = pd.DataFrame({'time_control': time_control})\n",
        "time_control_df.info()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sJR_FiBcNK04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_distribution(time_control, \"count\",'Time control per game ID', 'Time control')"
      ],
      "metadata": {
        "id": "v4kGkBEuet23",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Too few blitz games to the point they are not even appreciated"
      ],
      "metadata": {
        "id": "yc1zgMW9hal-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blitz = [i for i in time_control if i == 'Blitz']\n",
        "print('Number of blitz games:', len(blitz))"
      ],
      "metadata": {
        "id": "vhXElTvMg_8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very few blitz games"
      ],
      "metadata": {
        "id": "zNAR7PWthiU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "games_unique = pd.concat([games_unique, time_control_df], axis=1)\n",
        "games_unique.info()"
      ],
      "metadata": {
        "id": "lVtAENfjcWmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_unique.head()"
      ],
      "metadata": {
        "id": "kFzKsGLac4kx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accordint to the stats report, not all games with a winner status of draw have a winner value of draw"
      ],
      "metadata": {
        "id": "xqTUGAch1sN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "draws = games_unique[['victory_status','winner']]\n",
        "draws = draws[games_unique['winner'] == 'draw']\n",
        "not_draw = draws[draws['victory_status'] != 'draw']\n",
        "plot_distribution(draws['victory_status'], 'count', 'Victory status of draws', 'Victory Status')"
      ],
      "metadata": {
        "id": "A2GlTwgMtjnd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The alternative victory status  of drawed games is out of time, which makes sense since the game can result in a draw by insufficient winning material even when running out of time, so it is not a recording error"
      ],
      "metadata": {
        "id": "l9i-7ydE2GCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, I will also add the rating difference as a feature to see if it is useful for the predictions"
      ],
      "metadata": {
        "id": "f2pVSNyGMwAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "games_unique['rating_diff'] = games_unique['white_rating'] - games_unique['black_rating']\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Violin plot with boxplot and points\n",
        "sns.violinplot(y=games_unique['rating_diff'], inner=None, color=\"lightblue\")  # violin\n",
        "#sns.boxplot(y=games_unique['rating_diff'], width=0.1, color=\"white\")          # boxplot\n",
        "\n",
        "plt.title(\"Rating difference violing plot\")\n",
        "plt.ylabel(\"Rating difference\")\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "d0mAZ2cxNChX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical rating difference\n",
        "plot_winner_by(games_unique, 'rating_diff', title=\"Result of games by higher rating player in rating\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7H_O5dBIbsWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def define_train_test(df):\n",
        "    #Defines predictors and class variable and returns the train and test datasets\n",
        "    #If submission = True, it returns X and y without splitting since it will be used for training new dataset\n",
        "\n",
        "    target = 'winner'\n",
        "    y = df[target]\n",
        "    X = df.drop(target, axis = 1)\n",
        "\n",
        "    #80/20 split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "    print('Shape of Data (20%)')\n",
        "    print(\"X_train shape : \", X_train.shape)\n",
        "    print(\"y_train shape : \", y_train.shape)\n",
        "    print(\"X_test shape : \", X_test.shape)\n",
        "    print(\"y_test shape : \", y_test.shape)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "games_preprocessed = games_unique.drop(['id',\n",
        "                                        'turns',\n",
        "                                        'increment_code',\n",
        "                                        'victory_status',\n",
        "                                        'white_id',\n",
        "                                        'black_id',\n",
        "                                        'moves',\n",
        "                                        'opening_eco',\n",
        "                                        'opening_name',\n",
        "                                        'opening_ply'], axis = 1)\n",
        "X_train, X_test, y_train, y_test = define_train_test(games_preprocessed)"
      ],
      "metadata": {
        "id": "FobLBjTK89JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PorGPRdtM2E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_preprocessed.info()\n",
        "X_train.info()"
      ],
      "metadata": {
        "id": "tpYlbfSUA8qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_increment = X_train.drop(columns=['time_control'])\n",
        "test_increment = X_test.drop(columns=['time_control'])"
      ],
      "metadata": {
        "id": "FkXU_l9dDAIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "all_labels = pd.concat([y_train, y_test]).astype(str).unique()\n",
        "le.fit(all_labels)\n",
        "\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n"
      ],
      "metadata": {
        "id": "U0YpZyHuZvxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "\n",
        "def logging_callback(study, frozen_trial):\n",
        "    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n",
        "    if previous_best_value != study.best_value:\n",
        "        study.set_user_attr(\"previous_best_value\", study.best_value)\n",
        "        print(\n",
        "            \"Trial {} finished with best value: {} and parameters: {}. \".format(\n",
        "            frozen_trial.number,\n",
        "            frozen_trial.value,\n",
        "            frozen_trial.params,\n",
        "            )\n",
        "        )\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    def rf_model(trial):\n",
        "        #Objective function for bayesian hyperparameter optimization of a\n",
        "        #Random Forest classifier using Optuna\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 250),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
        "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 10, 30),\n",
        "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\",  5, 20),\n",
        "            \"max_features\": trial.suggest_int(\"max_features\", 1,5),\n",
        "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
        "            \"random_state\": 42\n",
        "\n",
        "        }\n",
        "        model = RandomForestClassifier(**params, n_jobs=-1)\n",
        "        return model\n",
        "\n",
        "    def lgbm_model(trial):\n",
        "        #Objective function for bayesian hyperparameter optimization of a\n",
        "        #Light Gradient-Boosting Machine using Optuna\n",
        "        params = {\n",
        "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1000),\n",
        "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1000),\n",
        "            \"random_state\": 42,\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "        }\n",
        "        model = LGBMClassifier(**params, verbose=-1)\n",
        "        return model\n",
        "\n",
        "    def xgb_model(trial):\n",
        "        #Objective function for bayesian hyperparameter optimization of an\n",
        "        #eXtreme Gradient Boostin classifier using Optuna\n",
        "        params = {\n",
        "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 1000),\n",
        "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 1000),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "            \"eval_metric\": \"logloss\",\n",
        "            \"random_state\": 42\n",
        "        }\n",
        "        model = XGBClassifier(**params)\n",
        "        return model\n",
        "\n",
        "    # Select which model to use\n",
        "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"XGBoost\", \"Light Gradient-Boosting Machine\"])\n",
        "\n",
        "    if model_name == \"RandomForest\":\n",
        "        model = rf_model(trial)\n",
        "    elif model_name == \"XGBoost\":\n",
        "        model = xgb_model(trial)\n",
        "    else:\n",
        "        model = lgbm_model(trial)\n",
        "\n",
        "    # Evaluate with cross-validation\n",
        "    recall_scorer = make_scorer(recall_score, average='micro')\n",
        "    score = cross_val_score(model, train_increment, y_train, cv=5, scoring=recall_scorer).mean()\n",
        "    return score\n",
        "\n",
        "\n",
        "#Create optuna study\n",
        "sampler = TPESampler(seed=10)\n",
        "study_increment = optuna.create_study(direction=\"maximize\", sampler = sampler)\n",
        "#Optimize study\n",
        "study_increment.optimize(objective, n_trials=200, callbacks = [logging_callback])\n",
        "print(f\"Best precision: {study_increment.best_value:.4f}\")\n",
        "best_params = study_increment.best_params.copy()\n",
        "print(f\"Best hyperparameters: {best_params}\")\n"
      ],
      "metadata": {
        "id": "ytusC_WiVCeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_name = best_params.pop(\"model\")\n",
        "\n",
        "if best_model_name == \"RandomForest\":\n",
        "    best_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)\n",
        "elif best_model_name == \"XGBoost\":\n",
        "    best_model = XGBClassifier(**best_params, eval_metric=\"logloss\", random_state=42)\n",
        "else:  # Light Gradient-Boosting Machine\n",
        "    best_model = LGBMClassifier(**best_params, max_iter=1000, random_state=42)\n",
        "\n",
        "best_model.fit(train_increment, y_train)\n",
        "test_prec = best_model.score(test_increment, y_test)\n",
        "print(f\"Best model: {best_model_name}\")\n",
        "print(f\"Test set precision: {test_prec:.4f}\")"
      ],
      "metadata": {
        "id": "kY6QTni4YiiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming best_model is already fitted (RandomForest, XGB, or LGBM)\n",
        "importances = best_model.feature_importances_\n",
        "features = train_increment.columns\n",
        "\n",
        "#Normalize importance\n",
        "importances = importances / importances.sum()\n",
        "\n",
        "# Put into a DataFrame for easy sorting\n",
        "feat_imp = pd.DataFrame({\n",
        "    \"Feature\": features,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(feat_imp[\"Feature\"], feat_imp[\"Importance\"], color=\"skyblue\")\n",
        "plt.gca().invert_yaxis()  # Most important at the top\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vR2WOdE6xwuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_classification_metrics(y_true, y_pred, labels=None):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix and print classification metrics.\n",
        "    \"\"\"\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    disp.plot(cmap='Blues', ax=ax, colorbar=False)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification report:\\n\")\n",
        "    print(classification_report(y_true, y_pred, target_names=labels))\n",
        "y_pred = best_model.predict(test_increment)\n",
        "labels = ['white', 'draw', 'black']\n",
        "\n",
        "# Decode\n",
        "y_test_decoded = le.inverse_transform(y_test)\n",
        "y_pred_decoded = le.inverse_transform(y_pred)\n",
        "\n",
        "# Now it will match\n",
        "plot_classification_metrics(y_test_decoded, y_pred_decoded, labels=['white','draw','black'])\n",
        "\n",
        "plot_classification_metrics(y_test, y_pred, labels=labels)\n"
      ],
      "metadata": {
        "id": "ABI4i_oapPz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def plot_multiclass_roc(y_true, y_pred_proba, classes):\n",
        "    \"\"\"\n",
        "    Plots ROC curves for multi-class classification.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: array-like of true labels (strings or numbers)\n",
        "    - y_pred_proba: predicted probabilities (n_samples x n_classes)\n",
        "    - classes: list of class names (strings) corresponding to y_pred_proba columns\n",
        "    \"\"\"\n",
        "    # Encode y_true if necessary\n",
        "    le = LabelEncoder()\n",
        "    le.fit(classes)  # ensures classes order matches probabilities\n",
        "    y_true_encoded = le.transform(y_true)\n",
        "    classes_encoded = le.transform(classes)\n",
        "\n",
        "    # Binarize labels for multi-class ROC\n",
        "    y_true_bin = label_binarize(y_true_encoded, classes=classes_encoded)\n",
        "\n",
        "    # Number of classes\n",
        "    n_classes = len(classes)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'{classes[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    plt.plot([0,1], [0,1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Multi-class ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "# --- Example usage ---\n",
        "classes = ['white', 'draw', 'black']               # class names\n",
        "y_pred_proba = best_model.predict_proba(test_increment)   # predicted probabilities\n",
        "plot_multiclass_roc(y_test, y_pred_proba, classes)\n"
      ],
      "metadata": {
        "id": "3FDmU_vUpR2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "def plot_precision_recall_curve(y_true, y_prob, classes):\n",
        "    y_true_bin = label_binarize(y_true, classes=classes)\n",
        "    n_classes = len(classes)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_prob[:, i])\n",
        "        ap = average_precision_score(y_true_bin[:, i], y_prob[:, i])\n",
        "        plt.plot(recall, precision, lw=2, label=f'{classes[i]} (AP = {ap:.2f})')\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.show()\n",
        "classes = ['white', 'draw', 'black']\n",
        "y_pred_proba = best_model.predict_proba(test_increment)   # predicted probabilities\n",
        "plot_precision_recall_curve(y_test, y_pred_proba, classes)"
      ],
      "metadata": {
        "id": "6Mmh9pmBpVUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(y_test) - set(y_train))\n"
      ],
      "metadata": {
        "id": "TA_UT2TUti1O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}