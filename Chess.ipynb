{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xelaro2304/MSB1015-Scientific-Programming/blob/main/Chess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtKQFyDWKgCn"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs7cfMPJK0-H"
      },
      "source": [
        "## Package installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d-16AgAau3KU"
      },
      "outputs": [],
      "source": [
        "!pip install -U ydata-profiling\n",
        "!pip install berserk\n",
        "!pip install optuna\n",
        "!apt install stockfish -y\n",
        "!pip install python-chess\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3ssSwLaK3Wp"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7_5vMPhC1vZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import berserk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from ydata_profiling import ProfileReport\n",
        "import re\n",
        "import chess\n",
        "import chess.engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8GDwL3nK5gv"
      },
      "source": [
        "## Function definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSUO8GpUMUb9"
      },
      "outputs": [],
      "source": [
        "def plot_distribution(data, plot=\"hist\", title=None, label=None, bins=30, show_stats=True, normalize=False, log = False, alpha = 0.7):\n",
        "    \"\"\"\n",
        "    Plot a histogram (numeric) or count plot (categorical) for a single variable.\n",
        "\n",
        "    Parameters:\n",
        "    - data: array-like, the variable to plot\n",
        "    - plot: \"hist\" for histogram, \"count\" for categorical count plot\n",
        "    - title: optional plot title\n",
        "    - label: optional x-axis label\n",
        "    - bins: number of bins for histogram (hist only)\n",
        "    - show_stats: show mean/median/mode (only for histogram)\n",
        "    - normalize: bool, whether to normalize frequencies/counts (0-1 or percentages)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(6,6))\n",
        "\n",
        "    #Histogram plot\n",
        "    if plot == \"hist\":\n",
        "        stat_type = 'density' if normalize else 'count'\n",
        "        sns.histplot(data,\n",
        "                     bins=bins,\n",
        "                     kde=False,\n",
        "                     color=sns.color_palette(\"colorblind\")[0],\n",
        "                     stat=stat_type,\n",
        "                     alpha = alpha)\n",
        "\n",
        "        #Plot statistics\n",
        "\n",
        "        if show_stats:\n",
        "            mean_val = np.mean(data)\n",
        "            median_val = np.median(data)\n",
        "            mode_val = stats.mode(data, keepdims=True)[0][0]\n",
        "            plt.axvline(mean_val, color=sns.color_palette(\"colorblind\")[1], linestyle=\"--\", linewidth=2.5, label=f\"Mean = {mean_val:.2f}\")\n",
        "            plt.axvline(median_val, color=sns.color_palette(\"colorblind\")[6], linestyle=\"--\", linewidth=2.5, label=f\"Median = {median_val:.2f}\")\n",
        "            plt.axvline(mode_val, color=sns.color_palette(\"colorblind\")[3], linestyle=\"--\", linewidth=2.5, label=f\"Mode = {mode_val:.2f}\")\n",
        "            plt.legend()\n",
        "\n",
        "        # Change to axis to log-scale\n",
        "        if log:\n",
        "            plt.yscale('log')\n",
        "\n",
        "        plt.ylabel(\"Density\" if normalize else \"Frequency\")\n",
        "        plt.xlabel(label if label else \"Value\")\n",
        "\n",
        "    #Count plot\n",
        "    elif plot == \"count\":\n",
        "        counts = data.value_counts(normalize=normalize)\n",
        "        counts.plot(kind='bar', color=sns.color_palette(\"colorblind\", len(counts)))\n",
        "        plt.ylabel(\"Proportion\" if normalize else \"Count\")\n",
        "        plt.xlabel(label if label else \"Category\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"plot must be either 'hist' or 'count'\")\n",
        "\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_winner_by(df, col, title=None, ylabel=None):\n",
        "    \"\"\"\n",
        "    Plot horizontal percentage-stacked bar chart for chess game results.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing 'winner' and the numeric column.\n",
        "    - col: Column name to group by (e.g., 'start_time', 'increment', 'avg_rating').\n",
        "    - title: Plot title.\n",
        "    - ylabel: Label for y-axis.\n",
        "    \"\"\"\n",
        "    df_plot = df.copy()\n",
        "\n",
        "    # Define bins\n",
        "    if col == 'avg_rating':\n",
        "        bins = [-np.inf, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000,\n",
        "                2100, 2200, 2300, 2400, np.inf]\n",
        "        labels = [\"< 1000\",\"1000-1100\",\"1100-1200\",\"1200-1300\",\"1300-1400\",\"1400-1500\",\n",
        "                  \"1500-1600\",\"1600-1700\",\"1700-1800\",\"1800-1900\",\"1900-2000\",\"2000-2100\",\n",
        "                  \"2100-2200\",\"2200-2300\",\"2300-2400\",\"> 2400\"]\n",
        "        df_plot[col] = pd.cut(df_plot[col], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "    if col == 'rating_diff':\n",
        "        df_plot['rating_diff_c2'] = np.where(df_plot['rating_diff'] > 0,\n",
        "                                            \"White higher rating\", \"White not-higher rating\")\n",
        "        # Count per category and winner\n",
        "        count_df = df_plot.groupby(['rating_diff_c2', 'winner']).size().reset_index(name='count')\n",
        "        count_pivot = count_df.pivot(index='rating_diff_c2', columns='winner', values='count').fillna(0)\n",
        "\n",
        "        # Plot heatmap\n",
        "        plt.figure(figsize=(8,6))\n",
        "        sns.heatmap(count_pivot, annot=True, fmt='g', cmap='Greys', linewidths=0.8, linecolor='black', cbar=False,\n",
        "                    annot_kws={\"size\": 15})\n",
        "        plt.title(title if title else \"Result of games by categorical difference in rating\")\n",
        "        plt.xlabel(\"Colour of winner\")\n",
        "        plt.ylabel(\"Rating before game\")\n",
        "        plt.show()\n",
        "        return\n",
        "\n",
        "    # Count per category and winner\n",
        "    count_df = df_plot.groupby([col, 'winner']).size().reset_index(name='count')\n",
        "\n",
        "    #Pivot for stacked percentage plot\n",
        "    count_pivot = count_df.pivot(index=col, columns='winner', values='count').fillna(0)\n",
        "    count_pct = count_pivot.div(count_pivot.sum(axis=1), axis=0)\n",
        "    count_pct = count_pct[['white', 'draw', 'black']]  # desired stacking order\n",
        "\n",
        "    # Define colors\n",
        "    color_map = {'white': '#d9d9d9', 'draw': 'grey', 'black': 'black'}\n",
        "\n",
        "    # Sort descending by the column\n",
        "    count_pct = count_pct.sort_index(ascending=False)\n",
        "    count_pivot = count_pivot.loc[count_pct.index]\n",
        "\n",
        "    #Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    count_pct.plot(kind='barh', stacked=True,\n",
        "                   color=[color_map.get(c, 'grey') for c in count_pct.columns],\n",
        "                   alpha=0.95, width=1, edgecolor='black', ax=ax)\n",
        "\n",
        "    ax.set_xlabel(\"Share of wins\")\n",
        "    ax.set_ylabel(ylabel if ylabel else col)\n",
        "    ax.set_title(title if title else f\"Result of games by {col}\")\n",
        "    ax.legend(title=\"Colour of winner\")\n",
        "    ax.xaxis.set_major_formatter(plt.matplotlib.ticker.PercentFormatter(1.0))\n",
        "\n",
        "    #Add counts in the middle of each segment\n",
        "    for i, val in enumerate(count_pct.index):\n",
        "        left = 0\n",
        "        for winner in count_pct.columns:\n",
        "            frac = count_pct.loc[val, winner]\n",
        "            value = count_pivot.loc[val, winner]\n",
        "            if value > 0:\n",
        "                text_color = 'black' if winner == 'white' else 'white'\n",
        "                ax.text(left + frac/2, i, int(value), ha='center', va='center',\n",
        "                        color=text_color, fontsize=12)\n",
        "                left += frac\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjyx98haLGSL"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBYR4gK9pfsE"
      },
      "outputs": [],
      "source": [
        "#Download dataset\n",
        "url = 'https://docs.google.com/uc?export=download&id=1lBXYMdZtKdMm4AtGWjJFjmBygUtn8w5y&confirm=t'\n",
        "path = os.getcwd()\n",
        "output = path + '/games.csv'\n",
        "!wget -O \"{output}\" \"{url}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnpv2wWym0Sr"
      },
      "outputs": [],
      "source": [
        "#Load dataset\n",
        "games_df = pd.read_csv(output, sep=';')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0IkwkxSL2oo"
      },
      "source": [
        "# Exploratory data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucTG6wCLMO_w"
      },
      "source": [
        "## Initial analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "m4oBpkaYrGgR"
      },
      "outputs": [],
      "source": [
        "games_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16XBWDVzrHco"
      },
      "outputs": [],
      "source": [
        "games_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jySP37brgOC"
      },
      "outputs": [],
      "source": [
        "games_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAejUC_UrqlN"
      },
      "outputs": [],
      "source": [
        "games_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LTJhQ2ZrynZ"
      },
      "outputs": [],
      "source": [
        "games_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En-xlsCEMSkg"
      },
      "source": [
        "## Data anomaly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkCoEP1DuE4P"
      },
      "source": [
        "Everything seems normal except for that minimum white rating, which will be inspected further"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Boxplot\n",
        "\n",
        "# Style settings\n",
        "sns.set(style=\"whitegrid\", palette=\"pastel\", font_scale=1.1)\n",
        "\n",
        "# Create the boxplot\n",
        "plt.figure(figsize=(4, 6))\n",
        "sns.boxplot(\n",
        "    y=games_df['white_rating'],\n",
        "    width=0.25,\n",
        "    color=\"#EAEAEA\",\n",
        "    boxprops=dict(facecolor=\"white\", edgecolor=\"#4C4C4C\", linewidth=1.2),\n",
        "    whiskerprops=dict(color=\"#4C4C4C\", linewidth=1.2),\n",
        "    capprops=dict(color=\"#4C4C4C\", linewidth=1.2),\n",
        "    medianprops=dict(color=\"#E74C3C\", linewidth=1.5),\n",
        "    flierprops=dict(marker='o', markersize=5, markerfacecolor='#E74C3C', alpha=0.6)\n",
        ")\n",
        "\n",
        "# Titles and labels\n",
        "plt.title(\"Distribution of White Player Ratings\", fontsize=13, weight='bold')\n",
        "plt.ylabel(\"White Rating\", fontsize=11)\n",
        "plt.xlabel(\"\")  # remove x-axis label\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
        "sns.despine(left=True, bottom=True)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "kmvNXZlr0SO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erp2GE1W3HXv"
      },
      "outputs": [],
      "source": [
        "#Df with only negative rating games\n",
        "negative_rating = games_df[\"white_rating\"]\n",
        "negative_rating = negative_rating[negative_rating < 0]\n",
        "print('Number of negative values:', len(negative_rating))\n",
        "negative_rating.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfqNWvT5Gh1"
      },
      "source": [
        "There is another game with a negative value for a rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1cIctKK3N9i"
      },
      "outputs": [],
      "source": [
        "negative_rating_indices = list(negative_rating.index)\n",
        "negative_rating_info = games_df.iloc[list(negative_rating_indices),]\n",
        "negative_rating_info.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DbSoeK_7RpJ"
      },
      "source": [
        "Will check the original values of the game by fetching it with game ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8fBU10b7XGp"
      },
      "outputs": [],
      "source": [
        "#Connect to Lichess API\n",
        "token = os.environ.get(\"BERSERK_TOKEN\")\n",
        "\n",
        "session = berserk.TokenSession(token)\n",
        "\n",
        "client = berserk.Client(session)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WALu11H1-Leu"
      },
      "outputs": [],
      "source": [
        "negative_rating_games = list(negative_rating_info[\"id\"])\n",
        "corrected_ratings = []\n",
        "for g in negative_rating_games:\n",
        "    game = client.games.export(g, as_pgn=True)\n",
        "    print(game)\n",
        "    game = game.split('\\n')\n",
        "    corrected_ratings.append(int(game[9][11:15]))\n",
        "print(corrected_ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E1vBf-sbMBYX"
      },
      "outputs": [],
      "source": [
        "#Correct ratings\n",
        "games_positive_rtg = games_df.copy()\n",
        "games_positive_rtg.loc[negative_rating_indices, 'white_rating'] = corrected_ratings\n",
        "games_positive_rtg.loc[negative_rating_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCFbJmkNMklL"
      },
      "source": [
        "## Data report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9wHgeX7usKa"
      },
      "outputs": [],
      "source": [
        "#Make report of dataframe\n",
        "profile = ProfileReport(games_df,title=\"Games report\")\n",
        "\n",
        "profile.to_file(\"games_report.html\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RUDJgnUhwuqD"
      },
      "outputs": [],
      "source": [
        "#Display report\n",
        "from IPython.display import HTML\n",
        "\n",
        "# show an HTML file inside the notebook\n",
        "HTML(filename=\"games_report.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syoyH4KeMs4j"
      },
      "source": [
        "## Player ratings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHHEwJ5Zc6n3"
      },
      "outputs": [],
      "source": [
        "plt.hexbin(games_df['white_rating'], games_df['black_rating'], gridsize=20, cmap='viridis')\n",
        "plt.colorbar(label=\"Number of games\")\n",
        "plt.xlabel(\"White rating\")\n",
        "plt.ylabel(\"Black rating\")\n",
        "plt.title(\"Player ratings heatmap\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e1VmjViPlms"
      },
      "source": [
        "## Average ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V8xezjJAYcjj"
      },
      "outputs": [],
      "source": [
        "# Average rating\n",
        "avg_rating = games_df\n",
        "games_df['avg_rating'] = (games_df['white_rating'] + games_df['black_rating']) / 2\n",
        "plot_winner_by(games_df, 'avg_rating', title=\"Result of games by average rating\", ylabel=\"Average rating\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy_ElO8DP6Gr"
      },
      "source": [
        "## Exploratory analysis conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP_LZbkvhbTt"
      },
      "source": [
        "Things to notice:\n",
        "\n",
        "1.   There seem to be some duplicated instances\n",
        "1.   There are 400 unique increment codes, which seems problematic to use for classification\n",
        "2.   The number of draws in winner is higher than the number of draws in victory status, will need to check that\n",
        "1.   Winner classes are somewhat balanced, except for the amount of draws\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ky7PYJHQA_2"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfdawJFIQEdI"
      },
      "source": [
        "## Duplicated instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HNEu2zn2pvEx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "game_ids = games_positive_rtg['id']\n",
        "print('Number of unique records:', len(games_positive_rtg['id'].unique()))\n",
        "duplicate_counts = games_positive_rtg['id'].value_counts()\n",
        "duplicate_ids = list(duplicate_counts[duplicate_counts > 1].index)\n",
        "duplicate_counts = duplicate_counts[duplicate_counts > 1]\n",
        "print('Total number of duplicated records:', sum(duplicate_counts))\n",
        "print('Number of records duplicated:', len(duplicate_counts))\n",
        "print('Duplicated ids:', duplicate_ids)\n",
        "\n",
        "plot_distribution(duplicate_counts.values, 'hist', 'Number of Duplicates per Game ID', 'Amount of times duplicated', show_stats=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckVZP3v6xAm4"
      },
      "source": [
        "Out of the 20,058 records, 19113 are unique, but it is detecting only 813 replicates instead of 945\n",
        "\n",
        "Repetition is mainly occuring in duplicates, although some of them are repeated 3-5 times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHqH3AOnLlHK"
      },
      "outputs": [],
      "source": [
        "#Remove duplicates\n",
        "games_unique = games_positive_rtg.drop_duplicates(keep='first')\n",
        "print(f\"Original rows: {len(games_positive_rtg)}, After removing duplicates: {len(games_unique)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REPNy9IoxK0I"
      },
      "source": [
        "Trying to remove duplicates values only removes ~400 of them, so I'll inspect further"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uT7ZuCGpxvPL"
      },
      "outputs": [],
      "source": [
        "duplicate_sample = games_positive_rtg[games_positive_rtg['id'].isin(duplicate_ids[0:4])]\n",
        "duplicate_sample.sort_values(by='id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikImkRV6z3Y_"
      },
      "source": [
        "Some of the repeated instances have distinct values of \"created_at\" and \"last_move_at\", so I'll try removing it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLMrIWRL0QJj"
      },
      "outputs": [],
      "source": [
        "#Remove creation time and last move columns\n",
        "games_time_dropped = games_positive_rtg.drop(columns=['created_at', 'last_move_at'])\n",
        "\n",
        "#Remove duplicates\n",
        "games_unique = games_time_dropped.drop_duplicates(keep='first').reset_index(drop=True)\n",
        "print(f\"Original rows: {len(games_time_dropped)}, After removing duplicates: {len(games_unique)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPmcgL1z1-3r"
      },
      "source": [
        "All duplicates removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPzDDodQR9CJ"
      },
      "source": [
        "## Convert increment codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXTlPx3LUImw"
      },
      "source": [
        "I'll try to handle the increment code in two ways:\n",
        "\n",
        "\n",
        "1.   Separate time into minutes and time increment per move\n",
        "2.   Classify each increment code into a time control\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqnXu79ySX-4"
      },
      "source": [
        "### Convert into initial time and increment move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMOMe-v5P5y9"
      },
      "outputs": [],
      "source": [
        "increment_code = games_unique['increment_code']\n",
        "increment_code_split = [time.split('+') for time in increment_code]\n",
        "print('Splitted increment codes:', increment_code_split)\n",
        "\n",
        "#As minutes and increment\n",
        "\n",
        "#Initial time\n",
        "start_time = [int(minutes[0]) for minutes in increment_code_split]\n",
        "print('Starting time in minutes:', start_time)\n",
        "\n",
        "#Increment\n",
        "increment = [int(seconds[1]) for seconds in increment_code_split]\n",
        "print('Increment in seconds:', increment)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ekjG5XXoMg_u"
      },
      "outputs": [],
      "source": [
        "plot_distribution(start_time, 'hist', 'Starting time distribution', 'Minutes', show_stats=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xBDHJwavMn5I"
      },
      "outputs": [],
      "source": [
        "plot_distribution(increment, 'hist', 'Increment distribution', 'Seconds', show_stats=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTdeHlQuOpJv"
      },
      "outputs": [],
      "source": [
        "start_time_df = pd.DataFrame(start_time, columns=['start_time'])\n",
        "increment_df = pd.DataFrame(increment, columns=['increment'])\n",
        "#check if there are games with 0 < start time < 1\n",
        "under_minute = ((start_time_df < 1) & (start_time_df > 0)).sum()\n",
        "print('Games with less than 1 minute of start time:', under_minute.iloc[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaBIu2LLM9D_"
      },
      "source": [
        "Most games are finish (no increment) and have 10 minutes as start time, with no game starting with less than a minute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lEQ3Lu1eSiUs"
      },
      "outputs": [],
      "source": [
        "#Add new features to df\n",
        "games_unique = pd.concat([games_unique, start_time_df], axis = 1)\n",
        "games_unique = pd.concat([games_unique, increment_df], axis = 1)\n",
        "games_unique.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aRkFW_dxWMPn"
      },
      "outputs": [],
      "source": [
        "plot_winner_by(games_unique, 'start_time', title=\"Result of games by start time\", ylabel=\"Start time (minutes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pMIQjcH0WNwl"
      },
      "outputs": [],
      "source": [
        "plot_winner_by(games_unique, 'increment', title=\"Result of games by increment\", ylabel=\"Increment (seconds)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J57HIoVwS2Au"
      },
      "source": [
        "### Convert into increment codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X437Tun_NLgU"
      },
      "source": [
        "According to the data source, [Lichess](https://lichess.org/faq#time-controls), time controls are decided assuming a game length of 40 moves and assigning the following categories depending on the duration:\n",
        "\n",
        "    ≤ 29s = UltraBullet\n",
        "    ≤ 179s = Bullet\n",
        "    ≤ 479s = Blitz\n",
        "    ≤ 1499s = Rapid\n",
        "    ≥ 1500s = Classical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJR_FiBcNK04"
      },
      "outputs": [],
      "source": [
        "def set_time_control(minutes, increment):\n",
        "    \"\"\"\n",
        "    Set time control considering a 40 move game length\n",
        "    minutes: Int, initial time of game\n",
        "    increment: Int, time added per move\n",
        "    \"\"\"\n",
        "    total_time = minutes*60+increment*40\n",
        "    if total_time <= 29:\n",
        "        return 'UltraBullet'\n",
        "    elif total_time <= 179:\n",
        "        return 'Bullet'\n",
        "    elif total_time <= 479:\n",
        "        return 'Blitz'\n",
        "    elif total_time <= 1499:\n",
        "        return 'Rapid'\n",
        "    else:\n",
        "        return 'Classical'\n",
        "\n",
        "time_control = games_unique.apply(lambda x: set_time_control(x['start_time'], x['increment']), axis=1)\n",
        "time_control_df = pd.DataFrame({'time_control': time_control})\n",
        "time_control_df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v4kGkBEuet23"
      },
      "outputs": [],
      "source": [
        "plot_distribution(time_control, \"count\",'Time control count', 'Time control')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc1zgMW9hal-"
      },
      "source": [
        "Too few blitz games to the point they are not even appreciated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhXElTvMg_8n"
      },
      "outputs": [],
      "source": [
        "blitz = [i for i in time_control if i == 'Blitz']\n",
        "print('Number of blitz games:', len(blitz))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNAR7PWthiU7"
      },
      "source": [
        "Very few blitz games\n",
        "Since there are too few it would cause troubles during the training and testing, so I dont consider this approach to be viable any longer and will not proceed with it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpP3reHmU-IZ"
      },
      "source": [
        "## Analysis of drawed games"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqTUGAch1sN8"
      },
      "source": [
        "Accordint to the stats report, not all games with a winner status of draw have a winner value of draw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "A2GlTwgMtjnd"
      },
      "outputs": [],
      "source": [
        "#Analyze drawed games\n",
        "draws = games_unique[['victory_status','winner']]\n",
        "draws = draws[games_unique['winner'] == 'draw']\n",
        "not_draw = draws[draws['victory_status'] != 'draw']\n",
        "plot_distribution(draws['victory_status'], 'count', 'Victory status of draws', 'Victory Status')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9i-7ydE2GCe"
      },
      "source": [
        "The alternative victory status  of drawed games is out of time, which makes sense since the game can result in a draw by insufficient winning material even when running out of time, so it is not a recording error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi_PrES9XUyN"
      },
      "source": [
        "## Rating difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2pVSNyGMwAP"
      },
      "source": [
        "I will also add the rating difference as a feature to see if it is useful for the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d0mAZ2cxNChX"
      },
      "outputs": [],
      "source": [
        "#Rating difference\n",
        "games_unique['rating_diff'] = games_unique['white_rating'] - games_unique['black_rating']\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Violin plot with boxplot and points\n",
        "sns.violinplot(y=games_unique['rating_diff'], inner=None, color=\"lightblue\")  # violin\n",
        "sns.boxplot(y=games_unique['rating_diff'], width=0.1, color=\"white\", fliersize=0)\n",
        "\n",
        "plt.title(\"Rating difference violing plot\")\n",
        "plt.ylabel(\"Rating difference\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCVs8dKxXAx1"
      },
      "outputs": [],
      "source": [
        "plot_distribution(games_unique['rating_diff'], 'hist', 'Rating difference distribution', 'Rating difference', show_stats=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7H_O5dBIbsWN"
      },
      "outputs": [],
      "source": [
        "# Categorical rating difference\n",
        "plot_winner_by(games_unique, 'rating_diff', title=\"Result of games by higher rating player in rating\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIB6Nh04VgdM"
      },
      "source": [
        "## Game Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQRgvnp8YJRt"
      },
      "source": [
        "Finally,as a simple approach to implement game information I am getting the evaluation of the position during the middle game from the algebraic notation of the game"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKGS3nxd7DqL"
      },
      "source": [
        "### Removal of short games"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSvzIFjj7Hgb"
      },
      "source": [
        "To avoid having a short game in which one of the players made a sudden abandonment or ran out of time I will filter all the games that did not reach left the opening phase and not entered the middle game\n",
        "\n",
        "While there is no clear division between the end of the opening and the start of the middle game, I will assume that middlegame starts after move 15 (30 turns) and remove games shorter than 20 moves to allow some middle game play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7bCUkg2t03V"
      },
      "outputs": [],
      "source": [
        "plot_distribution(games_unique['turns'], 'hist', 'Number of moves per game', 'Number of moves', bins=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2oSuwxW8Q3W"
      },
      "outputs": [],
      "source": [
        "# Keep only rows with at least 40 turns\n",
        "games_long = games_unique.copy()\n",
        "games_long = games_long[games_long['turns'] >= 40]\n",
        "\n",
        "games_long = games_long.reset_index(drop=True)\n",
        "games_long.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHR5vAbO9f2M"
      },
      "source": [
        "### Game evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3ayMPp5unUe"
      },
      "outputs": [],
      "source": [
        "#Load engine (Stockfish)\n",
        "engine = chess.engine.SimpleEngine.popen_uci(\"/usr/games/stockfish\")\n",
        "def game_eval(moves, idx=None, move_number=20, depth=10):\n",
        "    \"\"\"\n",
        "    Evaluate final position from White's perspective (positive = White better)\n",
        "    as centipawn evaluation. Handles notation cleanup, invalid moves,\n",
        "    and malformed strings.\n",
        "    \"\"\"\n",
        "\n",
        "    #Print when 100 games have been analyzed\n",
        "    if idx is not None and idx % 100 == 0:\n",
        "        print(f\"{idx} rows analyzed\")\n",
        "\n",
        "    # Clean move text: remove numbers, results, and dots\n",
        "    moves = re.sub(r\"\\d+\\.\", \"\", moves)\n",
        "    moves = re.sub(r\"(1-0|0-1|1/2-1/2|\\*)\", \"\", moves)\n",
        "    moves = moves.strip()\n",
        "\n",
        "    board = chess.Board()\n",
        "    target_plies = move_number * 2  # each move = 2 half-moves\n",
        "    count = 0\n",
        "\n",
        "    # Try to play all moves\n",
        "    for move in moves.split():\n",
        "        try:\n",
        "            board.push_san(move)\n",
        "            count += 1\n",
        "        except Exception:\n",
        "            # If illegal move return none\n",
        "            return None\n",
        "\n",
        "        if count >= target_plies:\n",
        "            break\n",
        "\n",
        "    # Run engine analysis\n",
        "    info = engine.analyse(board, chess.engine.Limit(depth=depth))\n",
        "    score = info[\"score\"].pov(chess.WHITE).score(mate_score=20000)\n",
        "    return score\n",
        "\n",
        "#Evaluate games\n",
        "games_processed = games_long.copy()\n",
        "games_processed[f\"evaluation\"] = [\n",
        "    game_eval(moves, idx=i) for i, moves in enumerate(games_processed[\"moves\"])\n",
        "]\n",
        "engine.quit()\n",
        "games_processed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KoExYVOzxaZ"
      },
      "outputs": [],
      "source": [
        "plot_distribution(games_processed['evaluation'], 'hist', 'Engine Evaluation distribution', 'Engine Evaluation', bins = 60, log =True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Games with absolute evaluation of ~20k would lead to data leakage since it is a way to interpret the result of a game, so they will be removed"
      ],
      "metadata": {
        "id": "nMfCMMKTVP9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove games with checkmate evaluation\n",
        "games_processed = games_processed[games_processed['evaluation'].between(-10000, 10000)]\n",
        "games_processed = games_processed.reset_index(drop=True)\n",
        "plot_distribution(games_processed['evaluation'], 'hist', 'Engine Evaluation distribution', 'Engine Evaluation', bins = 60, log =True)"
      ],
      "metadata": {
        "id": "on5DxWnf49Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyWekfDInhT_"
      },
      "outputs": [],
      "source": [
        "#Visualize distribution by result\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.violinplot(\n",
        "    x='winner',\n",
        "    y='evaluation',\n",
        "    data=games_processed,\n",
        "    inner='quartile',\n",
        "    palette='colorblind'\n",
        ")\n",
        "plt.yscale('symlog', linthresh=100)\n",
        "plt.xlabel('Game Result')\n",
        "plt.ylabel('Engine Evaluation')\n",
        "plt.title('Distribution of Engine Evaluations by Game Result')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpgKol5nD90B"
      },
      "source": [
        "## Repeat exploratory analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QptKO72_f6Wg"
      },
      "outputs": [],
      "source": [
        "#Visualize report after the preprocessing\n",
        "profile = ProfileReport(games_processed,title=\"Games after processing report\")\n",
        "profile.to_file(\"games_processed_report.html\")\n",
        "HTML(filename=\"games_processed_report.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY-mXcqqVQcT"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vhq4j41YDsB"
      },
      "source": [
        "## Data split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwk7OTzRZeYV"
      },
      "source": [
        "Drop unused features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfdT4zSaZgn-"
      },
      "outputs": [],
      "source": [
        "games_selected = games_processed.drop(['id',\n",
        "                                        'turns',\n",
        "                                        'increment_code',\n",
        "                                        'victory_status',\n",
        "                                        'white_id',\n",
        "                                        'black_id',\n",
        "                                        'moves',\n",
        "                                        'opening_eco',\n",
        "                                        'opening_name',\n",
        "                                        'opening_ply'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FobLBjTK89JJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def define_train_test(df):\n",
        "    #Defines predictors and class variable and returns the train and test datasets\n",
        "\n",
        "    #Define target variable\n",
        "    target = 'winner'\n",
        "    y = df[target]\n",
        "    X = df.drop(target, axis = 1)\n",
        "\n",
        "    #80/20 split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state = 42)\n",
        "\n",
        "    print('Shape of Data (20%)')\n",
        "    print(\"X_train shape : \", X_train.shape)\n",
        "    print(\"y_train shape : \", y_train.shape)\n",
        "    print(\"X_test shape : \", X_test.shape)\n",
        "    print(\"y_test shape : \", y_test.shape)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = define_train_test(games_selected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpYlbfSUA8qh"
      },
      "outputs": [],
      "source": [
        "games_selected.info()\n",
        "X_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G50-CgipYLWh"
      },
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0YpZyHuZvxu"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def encode (y_tra, y_te):\n",
        "    #Encodes label df while also returning the encoder\n",
        "    encoder = LabelEncoder()\n",
        "    y_tra = encoder.fit_transform(y_tra)\n",
        "    y_te = encoder.transform(y_te)\n",
        "    return y_tra, y_te, encoder\n",
        "\n",
        "y_train, y_test, le = encode(y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(le.classes_)\n"
      ],
      "metadata": {
        "id": "T_re06nFu41Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZVa_aJ-YOyL"
      },
      "source": [
        "## Model optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytusC_WiVCeX"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import make_scorer, recall_score\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from optuna.samplers import TPESampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "def optimize_model(X_tr, y_tr, class_imbalance=None):\n",
        "    \"\"\"\n",
        "    Optimize RF, XGB, and LGBM models using Optuna with optional imbalance handling.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_tr : array-like\n",
        "        Training features.\n",
        "    y_tr : array-like\n",
        "        Training labels.\n",
        "    class_imbalance : str, optional\n",
        "        One of {\"balancing\", \"weights\", None}. Default None.\n",
        "        - \"balancing\": applies RandomOverSampler within CV folds.\n",
        "        - \"weights\": uses class weights within model parameters (only LGBM and RF).\n",
        "    \"\"\"\n",
        "\n",
        "    #Make the optimizer print each time a new optimum is found\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "    def logging_callback(study, frozen_trial):\n",
        "        prev_best = study.user_attrs.get(\"prev_best\", None)\n",
        "        if prev_best != study.best_value:\n",
        "            study.set_user_attr(\"prev_best\", study.best_value)\n",
        "            print(\n",
        "                f\"Trial {frozen_trial.number} finished with best value: \"\n",
        "                f\"{frozen_trial.value:.4f} and parameters: {frozen_trial.params}\"\n",
        "            )\n",
        "\n",
        "    def objective(trial, X, y):\n",
        "        # Compute class weights if requested\n",
        "        if class_imbalance == \"weights\":\n",
        "            classes = np.unique(y)\n",
        "            weights = compute_class_weight(\"balanced\", classes=classes, y=y)\n",
        "            class_weight_dict = {cls: w for cls, w in zip(classes, weights)}\n",
        "        else:\n",
        "            class_weight_dict = None\n",
        "\n",
        "        #Model definitions\n",
        "        def rf_model(trial):\n",
        "            params = {\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 250),\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
        "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 10, 30),\n",
        "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 5, 20),\n",
        "                \"max_features\": trial.suggest_int(\"max_features\", 1, 5),\n",
        "                \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
        "                \"random_state\": 42,\n",
        "                \"n_jobs\": -1\n",
        "            }\n",
        "            if class_imbalance == \"weights\":\n",
        "                params[\"class_weight\"] = class_weight_dict\n",
        "            return RandomForestClassifier(**params)\n",
        "\n",
        "        def lgbm_model(trial):\n",
        "            params = {\n",
        "                \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1000),\n",
        "                \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1000),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "                \"random_state\": 42,\n",
        "                \"verbose\": -1\n",
        "            }\n",
        "            if class_imbalance == \"weights\":\n",
        "                params[\"class_weight\"] = class_weight_dict\n",
        "            return LGBMClassifier(**params)\n",
        "\n",
        "        def xgb_model(trial):\n",
        "            params = {\n",
        "                \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 1000),\n",
        "                \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 1000),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "                \"eval_metric\": \"mlogloss\",\n",
        "                \"random_state\": 42,\n",
        "            }\n",
        "            return XGBClassifier(**params)\n",
        "\n",
        "        #Choose model\n",
        "        model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"XGBoost\", \"Light Gradient-Boosting Machine\"])\n",
        "        if model_name == \"RandomForest\":\n",
        "            base_model = rf_model(trial)\n",
        "        elif model_name == \"XGBoost\":\n",
        "            base_model = xgb_model(trial)\n",
        "        else:\n",
        "            base_model = lgbm_model(trial)\n",
        "\n",
        "        #Build pipeline (only if balancing)\n",
        "        if class_imbalance == \"balancing\":\n",
        "            model = Pipeline([\n",
        "                (\"oversample\", RandomOverSampler(sampling_strategy='minority',random_state=42)),\n",
        "                (\"clf\", base_model)\n",
        "            ])\n",
        "        else:\n",
        "            model = base_model\n",
        "\n",
        "        #Make stratified folds\n",
        "        stratified_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "        #Define objective (macro recall)\n",
        "        recall_scorer = make_scorer(recall_score, average='macro')\n",
        "\n",
        "        score = cross_val_score(model, X, y, cv=stratified_cv, scoring=recall_scorer).mean()\n",
        "        return score\n",
        "\n",
        "    #Set seed\n",
        "    sampler = TPESampler(seed=10)\n",
        "\n",
        "    #Run optuna study\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "    study.optimize(lambda trial: objective(trial, X_tr, y_tr), n_trials=200, callbacks=[logging_callback])\n",
        "\n",
        "    print(f\"Best averaged recall: {study.best_value:.4f}\")\n",
        "    best_params = study.best_params.copy()\n",
        "    print(f\"Best hyperparameters: {best_params}\")\n",
        "    return best_params\n",
        "\n",
        "\n",
        "optimized = optimize_model(X_train, y_train)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN-5CcVC20Im"
      },
      "source": [
        "## Fitting and predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY6QTni4YiiZ"
      },
      "outputs": [],
      "source": [
        "def model_predict(params, X_tr, X_te, y_tr, y_te):\n",
        "    \"\"\"\n",
        "    Train a machine learning classifier with specified hyperparameters and evaluate it.\n",
        "\n",
        "    Parameters\n",
        "\n",
        "    params : dict\n",
        "        Dictionary containing the model type under key \"model\" and its hyperparameters.\n",
        "        Example: {\"model\": \"RandomForest\", \"n_estimators\": 100, \"max_depth\": 5}\n",
        "    X_tr : array-like\n",
        "        Training feature matrix.\n",
        "    X_te : array-like\n",
        "        Test feature matrix.\n",
        "    y_tr : array-like\n",
        "        Training labels.\n",
        "    y_te : array-like\n",
        "        Test labels.\n",
        "\n",
        "    Returns\n",
        "\n",
        "    best_model : fitted classifier object\n",
        "        The trained model.\n",
        "    y_predictions : array\n",
        "        Predicted class labels on the test set.\n",
        "    y_probabilities : array\n",
        "        Predicted class probabilities on the test set.\n",
        "    \"\"\"\n",
        "    best_params = params.copy()\n",
        "\n",
        "    #Extract model type\n",
        "    best_model_name = best_params.pop(\"model\")\n",
        "\n",
        "    if best_model_name == \"RandomForest\":\n",
        "        best_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)\n",
        "    elif best_model_name == \"XGBoost\":\n",
        "        best_model = XGBClassifier(**best_params, eval_metric=\"logloss\", random_state=42)\n",
        "    else:  # Light Gradient-Boosting Machine\n",
        "        best_model = LGBMClassifier(**best_params, max_iter=1000, random_state=42)\n",
        "\n",
        "    #Train model\n",
        "    best_model.fit(X_tr, y_tr)\n",
        "    test_recall = best_model.score(X_te, y_te)\n",
        "    print(f\"Best model: {best_model_name}\")\n",
        "    print(f\"Test set score: {test_recall:.4f}\")\n",
        "    y_predictions = best_model.predict(X_te)\n",
        "    y_probabilities = best_model.predict_proba(X_te)\n",
        "    return best_model, y_predictions, y_probabilities\n",
        "\n",
        "fitted_model, y_pred, y_pred_proba = model_predict(optimized, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owsT_PP4Yewt"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHqGbys_YgeV"
      },
      "source": [
        "## Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR2WOdE6xwuQ"
      },
      "outputs": [],
      "source": [
        "def feature_importance(best_model, X):\n",
        "    \"\"\"\n",
        "    Calculate and plot feature importance\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    best_model : fitted classifier object\n",
        "        The trained model.\n",
        "    X : array-like\n",
        "        Feature matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    importances = best_model.feature_importances_\n",
        "    features = X.columns\n",
        "\n",
        "    #Normalize importance\n",
        "    importances = importances / importances.sum()\n",
        "\n",
        "    # Put into a DataFrame for easy sorting\n",
        "    feat_imp = pd.DataFrame({\n",
        "        \"Feature\": features,\n",
        "        \"Importance\": importances\n",
        "    }).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.barh(feat_imp[\"Feature\"], feat_imp[\"Importance\"], color=\"skyblue\")\n",
        "    plt.gca().invert_yaxis()  # Most important at the top\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.title(\"Feature Importance\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "feature_importance(fitted_model, X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUaFButlYkb4"
      },
      "source": [
        "## Classification metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABI4i_oapPz4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def classification_metrics(best_model, X, y, encoder):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix alongside overall and per-class classification metrics.\n",
        "\n",
        "    Parameters\n",
        "\n",
        "    best_model : fitted classifier object\n",
        "        The trained model.\n",
        "    X : array-like\n",
        "        Feature matrix.\n",
        "    y : array-like\n",
        "        True labels.\n",
        "    encoder : LabelEncoder\n",
        "        The fitted label encoder.\n",
        "    \"\"\"\n",
        "    # Predict encoded labels\n",
        "    y_pred = best_model.predict(X)\n",
        "\n",
        "    #Decode both true and predicted labels\n",
        "    y_true_decoded = encoder.inverse_transform(y)\n",
        "    y_pred_decoded = encoder.inverse_transform(y_pred)\n",
        "\n",
        "    labels = encoder.classes_\n",
        "\n",
        "    #Confusion Matrix\n",
        "    cm = confusion_matrix(y_true_decoded, y_pred_decoded, labels=labels, normalize='true')\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    disp.plot(cmap='Blues', ax=ax, colorbar=False)\n",
        "    plt.title(\"Confusion Matrix (Normalized by True Labels)\")\n",
        "    plt.show()\n",
        "\n",
        "    #Classification Report\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    report = classification_report(y_true_decoded, y_pred_decoded, target_names=labels, output_dict=True, zero_division=0)\n",
        "    print(classification_report(y_true_decoded, y_pred_decoded, target_names=labels, zero_division=0))\n",
        "\n",
        "    #Convert to DataFrame for plotting\n",
        "    report_df = pd.DataFrame(report).transpose().drop(['accuracy', 'macro avg', 'weighted avg'], errors='ignore')\n",
        "\n",
        "     #Performance Metrics\n",
        "    accuracy = accuracy_score(y_true_decoded, y_pred_decoded)\n",
        "    precision = precision_score(y_true_decoded, y_pred_decoded, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true_decoded, y_pred_decoded, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true_decoded, y_pred_decoded, average='weighted', zero_division=0)\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-score\": f1\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette=\"colorblind\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(\"Overall Performance Metrics\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    for i, v in enumerate(metrics.values()):\n",
        "        plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontweight='bold')\n",
        "    plt.show()\n",
        "\n",
        "    #Per-class Metrics Plot\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    report_df[['precision', 'recall', 'f1-score']].plot(\n",
        "        kind='bar', figsize=(10, 6), colormap='viridis', width=0.75\n",
        "    )\n",
        "    plt.title(\"Per-Class Performance Metrics\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.xticks(range(len(report_df)), report_df.index, rotation=45, ha='right')\n",
        "    plt.legend(title=\"Metric\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "classification_metrics(fitted_model, X_test, y_test, le)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPTsoMltYu0D"
      },
      "source": [
        "## ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCa9NavCdaRx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "\n",
        "def plot_multiclass_roc(y_true, y_proba, model, label_encoder):\n",
        "    \"\"\"\n",
        "    Plots ROC curves for multi-class classification.\n",
        "\n",
        "    Parameters:\n",
        "\n",
        "    y_true: encoded (numeric) labels\n",
        "    y_prob: predicted probabilities (n_samples × n_classes)\n",
        "    model: fitted classifier\n",
        "    label_encoder: fitted LabelEncoder\n",
        "    \"\"\"\n",
        "    # Use the encoder’s class order to ensure consistency\n",
        "    classes = label_encoder.classes_\n",
        "    n_classes = len(classes)\n",
        "\n",
        "    # Binarize the true labels according to the class order\n",
        "    y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
        "\n",
        "    # Colorblind-friendly palette\n",
        "    colors = sns.color_palette(\"colorblind\", n_classes)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Per-class ROC curves\n",
        "    for i, cls in enumerate(classes):\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_proba[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, lw=2, color=colors[i], label=f'{cls} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    # Macro-average ROC\n",
        "    all_fpr = np.unique(\n",
        "        np.concatenate([roc_curve(y_true_bin[:, i], y_proba[:, i])[0] for i in range(n_classes)])\n",
        "    )\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_proba[:, i])\n",
        "        mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
        "    mean_tpr /= n_classes\n",
        "    roc_auc_macro = auc(all_fpr, mean_tpr)\n",
        "    plt.plot(all_fpr, mean_tpr, color='navy', linestyle='-',\n",
        "             label=f'Macro-average ROC (AUC = {roc_auc_macro:.2f})', lw=2)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Multi-class ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_multiclass_roc(y_test, y_pred_proba, fitted_model, le)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae1uGFdEDgY7"
      },
      "source": [
        "## Precision Recall curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mmh9pmBpVUU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "def plot_precision_recall_curve(y_true, y_proba, model, label_encoder):\n",
        "    \"\"\"\n",
        "    Plots precision–recall curves for multi-class classification.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: encoded (numeric) labels\n",
        "    - y_prob: predicted probabilities (n_samples × n_classes)\n",
        "    - model: fitted classifier\n",
        "    - label_encoder: fitted LabelEncoder\n",
        "    \"\"\"\n",
        "    classes = label_encoder.classes_\n",
        "    n_classes = len(classes)\n",
        "\n",
        "    # Binarize true labels using the encoder’s order\n",
        "    y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
        "\n",
        "    # Colorblind friendly palette\n",
        "    colors = sns.color_palette(\"colorblind\", n_classes)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Per-class PR curves\n",
        "    for i, cls in enumerate(classes):\n",
        "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_proba[:, i])\n",
        "        ap = average_precision_score(y_true_bin[:, i], y_proba[:, i])\n",
        "        plt.plot(\n",
        "            recall, precision, lw=2, color=colors[i],\n",
        "            label=f'{cls} (AP = {ap:.2f})'\n",
        "        )\n",
        "\n",
        "    # Macro-average curve\n",
        "    all_recall = np.unique(np.concatenate([\n",
        "        precision_recall_curve(y_true_bin[:, i], y_proba[:, i])[1] for i in range(n_classes)\n",
        "    ]))\n",
        "    mean_precision = np.zeros_like(all_recall)\n",
        "    for i in range(n_classes):\n",
        "        p, r, _ = precision_recall_curve(y_true_bin[:, i], y_proba[:, i])\n",
        "        mean_precision += np.interp(all_recall, r[::-1], p[::-1])\n",
        "    mean_precision /= n_classes\n",
        "    mean_ap = np.mean([\n",
        "        average_precision_score(y_true_bin[:, i], y_proba[:, i]) for i in range(n_classes)\n",
        "    ])\n",
        "    plt.plot(\n",
        "        all_recall, mean_precision,\n",
        "        color='black', linestyle='--', lw=2,\n",
        "        label=f'Macro-average (AP = {mean_ap:.2f})'\n",
        "    )\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Multi-class Precision–Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_precision_recall_curve(y_test, y_pred_proba, fitted_model, le)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5pncpcpz9qm"
      },
      "source": [
        "# Reruns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rigwaozAy0Dy"
      },
      "source": [
        "I want to try to get some predictions of the draw class, so I will make a pipeline and try some approaches to address the imbalancing problem"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oversampling"
      ],
      "metadata": {
        "id": "IbXbCj-V3e_4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lET3XOGO0EUf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def run_model(df, imbalance = None):\n",
        "    \"\"\"\n",
        "    Full pipeline to train, evaluate, and visualize a machine learning classifier.\n",
        "\n",
        "    Parameters\n",
        "\n",
        "    df : pandas.DataFrame\n",
        "        Input dataframe containing features and target variable.\n",
        "    imbalance : str or None, optional\n",
        "        Method to handle class imbalance ('balancing' to apply random oversampler of minority class,\n",
        "        'weights' to apply classification weights, or None).\n",
        "\n",
        "    Returns\n",
        "\n",
        "    fitted_model_fun : classifier object\n",
        "        The trained model.\n",
        "    X_train_fun, X_test_fun : array-like\n",
        "        Training and test feature matrices.\n",
        "    y_train_fun, y_test_fun : array-like\n",
        "        Training and test target vectors (encoded if needed).\n",
        "    y_pred_fun : array-like\n",
        "        Predicted class labels for the test set.\n",
        "    y_pred_proba_fun : array-like\n",
        "        Predicted probabilities for the test set.\n",
        "    le_fun : LabelEncoder\n",
        "        Fitted label encoder to map class labels back to original values.\n",
        "    \"\"\"\n",
        "\n",
        "    X_train_fun, X_test_fun, y_train_fun, y_test_fun = define_train_test(df)\n",
        "    y_train_fun, y_test_fun, le_fun = encode(y_train_fun, y_test_fun)\n",
        "    optimized_fun = optimize_model(X_train_fun, y_train_fun, class_imbalance=imbalance)\n",
        "    fitted_model_fun, y_pred_fun, y_pred_proba_fun = model_predict(optimized_fun, X_train_fun, X_test_fun, y_train_fun, y_test_fun)\n",
        "    feature_importance(fitted_model_fun, X_train_fun)\n",
        "    classification_metrics(fitted_model_fun, X_test_fun, y_test_fun, le_fun)\n",
        "    plot_multiclass_roc(y_test_fun, y_pred_proba_fun, fitted_model_fun, le_fun)\n",
        "    plot_precision_recall_curve(y_test_fun, y_pred_proba_fun, fitted_model_fun, le_fun)\n",
        "\n",
        "    return fitted_model_fun, X_train_fun, X_test_fun, y_train_fun, y_test_fun, y_pred_fun, y_pred_proba_fun, le_fun\n",
        "\n",
        "fitted_model_v2, X_train_v2, X_test_v2, y_train_v2, y_test_v2, y_pred_v2, y_pred_proba_v2, le_v2 = run_model(games_selected, imbalance = 'balancing')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnOn1d-Ksx0d"
      },
      "source": [
        "Metrics are barely affected"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class weights"
      ],
      "metadata": {
        "id": "rm28O3sI5BE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_model_v3, X_train_v3, X_test_v3, y_train_v3, y_test_v3, y_pred_v3, y_pred_proba_v3, le_v3 = run_model(games_selected, imbalance = 'weights')"
      ],
      "metadata": {
        "id": "tAfeC9k25Clm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metagame data only"
      ],
      "metadata": {
        "id": "4V5lL57z6F0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I want to see the performance with pure metagame data"
      ],
      "metadata": {
        "id": "CU6D0RTzHRyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove evaluatio\n",
        "games_selected_metadata = games_selected.drop(['evaluation'], axis = 1)\n",
        "games_selected_metadata.head()"
      ],
      "metadata": {
        "id": "KYy-w7fuHXa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_model_v4, X_train_v4, X_test_v4, y_train_v4, y_test_v4, y_pred_v4, y_pred_proba_v4, le_v4 = run_model(games_selected_metadata)"
      ],
      "metadata": {
        "id": "UyWjIU8FHhxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqkyTGK/jRLpb7e90wl0DZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}