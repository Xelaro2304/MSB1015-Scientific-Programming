{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8qNIl5Wj6bO/bn1cp7v+b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xelaro2304/MSB1015-Scientific-Programming/blob/main/Chess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "VtKQFyDWKgCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package installation"
      ],
      "metadata": {
        "id": "Bs7cfMPJK0-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ydata-profiling\n",
        "!pip install berserk\n",
        "!pip install optuna\n",
        "!apt install stockfish -y\n",
        "!pip install python-chess"
      ],
      "metadata": {
        "id": "d-16AgAau3KU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages"
      ],
      "metadata": {
        "id": "w3ssSwLaK3Wp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7_5vMPhC1vZ"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os\n",
        "import berserk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from ydata_profiling import ProfileReport\n",
        "import re\n",
        "import chess\n",
        "import chess.engine"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function definitions"
      ],
      "metadata": {
        "id": "e8GDwL3nK5gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_distribution(data, plot=\"hist\", title=None, label=None, bins=30, show_stats=True, normalize=False, log = False, alpha = 0.7):\n",
        "    \"\"\"\n",
        "    Plot a histogram (numeric) or count plot (categorical) for a single variable.\n",
        "\n",
        "    Parameters:\n",
        "    - data: array-like, the variable to plot\n",
        "    - plot: \"hist\" for histogram, \"count\" for categorical count plot\n",
        "    - title: optional plot title\n",
        "    - label: optional x-axis label\n",
        "    - bins: number of bins for histogram\n",
        "    - show_stats: show mean/median/mode (only for histogram)\n",
        "    - normalize: bool, whether to normalize frequencies/counts (0-1 or percentages)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(6,6))\n",
        "\n",
        "    if plot == \"hist\":\n",
        "        stat_type = 'density' if normalize else 'count'\n",
        "        sns.histplot(data,\n",
        "                     bins=bins,\n",
        "                     kde=False,\n",
        "                     color=sns.color_palette(\"colorblind\")[0],\n",
        "                     stat=stat_type,\n",
        "                     alpha = alpha)\n",
        "\n",
        "        if show_stats:\n",
        "            mean_val = np.mean(data)\n",
        "            median_val = np.median(data)\n",
        "            mode_val = stats.mode(data, keepdims=True)[0][0]\n",
        "            plt.axvline(mean_val, color=sns.color_palette(\"colorblind\")[1], linestyle=\"--\", linewidth=2.5, label=f\"Mean = {mean_val:.2f}\")\n",
        "            plt.axvline(median_val, color=sns.color_palette(\"colorblind\")[6], linestyle=\"--\", linewidth=2.5, label=f\"Median = {median_val:.2f}\")\n",
        "            plt.axvline(mode_val, color=sns.color_palette(\"colorblind\")[3], linestyle=\"--\", linewidth=2.5, label=f\"Mode = {mode_val:.2f}\")\n",
        "            plt.legend()\n",
        "\n",
        "\n",
        "        if log:\n",
        "            plt.yscale('log')\n",
        "\n",
        "        plt.ylabel(\"Density\" if normalize else \"Frequency\")\n",
        "        plt.xlabel(label if label else \"Value\")\n",
        "\n",
        "    elif plot == \"count\":\n",
        "        counts = data.value_counts(normalize=normalize)\n",
        "        counts.plot(kind='bar', color=sns.color_palette(\"colorblind\", len(counts)))\n",
        "        plt.ylabel(\"Proportion\" if normalize else \"Count\")\n",
        "        plt.xlabel(label if label else \"Category\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"plot must be either 'hist' or 'count'\")\n",
        "\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_winner_by(df, col, title=None, ylabel=None):\n",
        "    \"\"\"\n",
        "    Plot horizontal percentage-stacked bar chart for chess game results.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing 'winner' and the numeric column.\n",
        "    - col: Column name to group by (e.g., 'start_time', 'increment', 'avg_rating').\n",
        "    - title: Plot title.\n",
        "    - ylabel: Label for y-axis.\n",
        "    \"\"\"\n",
        "    df_plot = df.copy()\n",
        "\n",
        "    if col == 'avg_rating':\n",
        "        bins = [-np.inf, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000,\n",
        "                2100, 2200, 2300, 2400, np.inf]\n",
        "        labels = [\"< 1000\",\"1000-1100\",\"1100-1200\",\"1200-1300\",\"1300-1400\",\"1400-1500\",\n",
        "                  \"1500-1600\",\"1600-1700\",\"1700-1800\",\"1800-1900\",\"1900-2000\",\"2000-2100\",\n",
        "                  \"2100-2200\",\"2200-2300\",\"2300-2400\",\"> 2400\"]\n",
        "        df_plot[col] = pd.cut(df_plot[col], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "    if col == 'rating_diff':\n",
        "        df_plot['rating_diff_c2'] = np.where(df_plot['rating_diff'] > 0,\n",
        "                                            \"White higher rating\", \"White not-higher rating\")\n",
        "        # Count per category and winner\n",
        "        count_df = df_plot.groupby(['rating_diff_c2', 'winner']).size().reset_index(name='count')\n",
        "        count_pivot = count_df.pivot(index='rating_diff_c2', columns='winner', values='count').fillna(0)\n",
        "\n",
        "        # Plot heatmap\n",
        "        plt.figure(figsize=(8,6))\n",
        "        sns.heatmap(count_pivot, annot=True, fmt='g', cmap='Greys', linewidths=0.8, linecolor='black', cbar=False,\n",
        "                    annot_kws={\"size\": 15})\n",
        "        plt.title(title if title else \"Result of games by categorical difference in rating\")\n",
        "        plt.xlabel(\"Colour of winner\")\n",
        "        plt.ylabel(\"Rating before game\")\n",
        "        plt.show()\n",
        "        return\n",
        "\n",
        "    # --- Count per category and winner ---\n",
        "    count_df = df_plot.groupby([col, 'winner']).size().reset_index(name='count')\n",
        "\n",
        "    # --- Pivot for stacked percentage plot ---\n",
        "    count_pivot = count_df.pivot(index=col, columns='winner', values='count').fillna(0)\n",
        "    count_pct = count_pivot.div(count_pivot.sum(axis=1), axis=0)\n",
        "    count_pct = count_pct[['white', 'draw', 'black']]  # desired stacking order\n",
        "\n",
        "    # --- Define colors ---\n",
        "    color_map = {'white': '#d9d9d9', 'draw': 'grey', 'black': 'black'}\n",
        "\n",
        "    # --- Sort descending by the column ---\n",
        "    count_pct = count_pct.sort_index(ascending=False)\n",
        "    count_pivot = count_pivot.loc[count_pct.index]\n",
        "\n",
        "    # --- Plot ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    count_pct.plot(kind='barh', stacked=True,\n",
        "                   color=[color_map.get(c, 'grey') for c in count_pct.columns],\n",
        "                   alpha=0.95, width=1, edgecolor='black', ax=ax)\n",
        "\n",
        "    ax.set_xlabel(\"Share of wins\")\n",
        "    ax.set_ylabel(ylabel if ylabel else col)\n",
        "    ax.set_title(title if title else f\"Result of games by {col}\")\n",
        "    ax.legend(title=\"Colour of winner\")\n",
        "    ax.xaxis.set_major_formatter(plt.matplotlib.ticker.PercentFormatter(1.0))\n",
        "\n",
        "    # --- Add counts in the middle of each segment ---\n",
        "    for i, val in enumerate(count_pct.index):\n",
        "        left = 0\n",
        "        for winner in count_pct.columns:\n",
        "            frac = count_pct.loc[val, winner]  # fraction for plotting\n",
        "            value = count_pivot.loc[val, winner]  # raw count\n",
        "            if value > 0:\n",
        "                text_color = 'black' if winner == 'white' else 'white'\n",
        "                ax.text(left + frac/2, i, int(value), ha='center', va='center',\n",
        "                        color=text_color, fontsize=12)\n",
        "                left += frac\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "MSUO8GpUMUb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading"
      ],
      "metadata": {
        "id": "vjyx98haLGSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://docs.google.com/uc?export=download&id=1lBXYMdZtKdMm4AtGWjJFjmBygUtn8w5y&confirm=t'\n",
        "path = os.getcwd()\n",
        "output = path + '/games.csv'\n",
        "!wget -O \"{output}\" \"{url}\""
      ],
      "metadata": {
        "id": "CBYR4gK9pfsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_df = pd.read_csv(output, sep=';')\n"
      ],
      "metadata": {
        "id": "mnpv2wWym0Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory data analysis"
      ],
      "metadata": {
        "id": "v0IkwkxSL2oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial analysis"
      ],
      "metadata": {
        "id": "ucTG6wCLMO_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "games_df.head()"
      ],
      "metadata": {
        "id": "m4oBpkaYrGgR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_df.shape"
      ],
      "metadata": {
        "id": "16XBWDVzrHco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_df.info()"
      ],
      "metadata": {
        "id": "0jySP37brgOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_df.isnull().sum()"
      ],
      "metadata": {
        "id": "JAejUC_UrqlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_df.describe()"
      ],
      "metadata": {
        "id": "1LTJhQ2ZrynZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data anomaly"
      ],
      "metadata": {
        "id": "En-xlsCEMSkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything seems normal except for that minimum white rating, which will be inspected further"
      ],
      "metadata": {
        "id": "IkCoEP1DuE4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_rating = games_df[\"white_rating\"]\n",
        "negative_rating = negative_rating[negative_rating < 0]\n",
        "print('Number of negative values:', len(negative_rating))\n",
        "negative_rating.head()"
      ],
      "metadata": {
        "id": "erp2GE1W3HXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is another game with a negative value for a rating"
      ],
      "metadata": {
        "id": "MKfqNWvT5Gh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_rating_indices = list(negative_rating.index)\n",
        "negative_rating_info = games_df.iloc[list(negative_rating_indices),]\n",
        "negative_rating_info.head()"
      ],
      "metadata": {
        "id": "d1cIctKK3N9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will check the original values of the game by fetching it with game ID"
      ],
      "metadata": {
        "id": "5DbSoeK_7RpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./token') as f:\n",
        "\n",
        "    token = f.read()\n",
        "    token = token.strip()\n",
        "\n",
        "\n",
        "session = berserk.TokenSession(token)\n",
        "\n",
        "client = berserk.Client(session)"
      ],
      "metadata": {
        "id": "_8fBU10b7XGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_rating_games = list(negative_rating_info[\"id\"])\n",
        "corrected_ratings = []\n",
        "for g in negative_rating_games:\n",
        "    game = client.games.export(g, as_pgn=True)\n",
        "    print(game)\n",
        "    game = game.split('\\n')\n",
        "    corrected_ratings.append(int(game[9][11:15]))\n",
        "print(corrected_ratings)"
      ],
      "metadata": {
        "id": "WALu11H1-Leu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_positive_rtg = games_df\n",
        "games_positive_rtg.loc[negative_rating_indices, 'white_rating'] = corrected_ratings\n",
        "games_positive_rtg.loc[negative_rating_indices]"
      ],
      "metadata": {
        "id": "E1vBf-sbMBYX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data report"
      ],
      "metadata": {
        "id": "yCFbJmkNMklL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(games_df,title=\"Games report\")\n",
        "\n",
        "profile.to_file(\"games_report.html\")\n"
      ],
      "metadata": {
        "id": "B9wHgeX7usKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!env BROWSER=firefox\n",
        "#!open games_report.html\n",
        "from IPython.display import HTML\n",
        "\n",
        "# show an HTML file inside the notebook\n",
        "HTML(filename=\"games_report.html\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RUDJgnUhwuqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Player ratings\n"
      ],
      "metadata": {
        "id": "syoyH4KeMs4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hexbin(games_df['white_rating'], games_df['black_rating'], gridsize=20, cmap='viridis')\n",
        "plt.colorbar(label=\"Number of games\")\n",
        "plt.xlabel(\"White rating\")\n",
        "plt.ylabel(\"Black rating\")\n",
        "plt.title(\"Player ratings heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RHHEwJ5Zc6n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average ratings"
      ],
      "metadata": {
        "id": "-e1VmjViPlms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average rating\n",
        "avg_rating = games_df\n",
        "games_df['avg_rating'] = (games_df['white_rating'] + games_df['black_rating']) / 2\n",
        "plot_winner_by(games_df, 'avg_rating', title=\"Result of games by average rating\", ylabel=\"Average rating\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V8xezjJAYcjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory analysis conclusions"
      ],
      "metadata": {
        "id": "Fy_ElO8DP6Gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things to notice:\n",
        "\n",
        "1.   There seem to be some duplicated instances\n",
        "1.   There are 400 unique increment codes, which seems problematic to use for classification\n",
        "2.   The number of draws in winner is higher than the number of draws in victory status, will need to check that\n",
        "1.   Winner classes are somewhat balanced, except for the amount of draws\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vP_LZbkvhbTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "4ky7PYJHQA_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplicated instances"
      ],
      "metadata": {
        "id": "sfdawJFIQEdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "game_ids = games_positive_rtg['id']\n",
        "#duplicates = [i for i in game_ids if game_ids.count(i) > 1]\n",
        "#print(duplicates)\n",
        "print('Number of unique records:', len(games_positive_rtg['id'].unique()))\n",
        "duplicate_counts = games_positive_rtg['id'].value_counts()\n",
        "duplicate_ids = list(duplicate_counts[duplicate_counts > 1].index)\n",
        "duplicate_counts = duplicate_counts[duplicate_counts > 1]\n",
        "print('Total number of duplicated records:', sum(duplicate_counts))\n",
        "print('Number of records duplicated:', len(duplicate_counts))\n",
        "print('Duplicated ids:', duplicate_ids)\n",
        "\n",
        "plot_distribution(duplicate_counts.values, 'hist', 'Number of Duplicates per Game ID', 'Amount of times duplicated', show_stats=False)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HNEu2zn2pvEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of the 20,058 records, 19113 are unique, but it is detecting only 813 replicates instead of 945\n",
        "\n",
        "Repetition is mainly occuring in duplicates, although some of them are repeated 3-5 times"
      ],
      "metadata": {
        "id": "ckVZP3v6xAm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "games_unique = games_positive_rtg.drop_duplicates(keep='first')\n",
        "print(f\"Original rows: {len(games_positive_rtg)}, After removing duplicates: {len(games_unique)}\")"
      ],
      "metadata": {
        "id": "nHqH3AOnLlHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to remove duplicates values only removes ~400 of them, so I'll inspect further"
      ],
      "metadata": {
        "id": "REPNy9IoxK0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_sample = games_positive_rtg[games_positive_rtg['id'].isin(duplicate_ids[0:4])]\n",
        "duplicate_sample.sort_values(by='id')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uT7ZuCGpxvPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the repeated instances have distinct values of \"created_at\" and \"last_move_at\", so I'll try removing it"
      ],
      "metadata": {
        "id": "ikImkRV6z3Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "games_time_dropped = games_positive_rtg.drop(columns=['created_at', 'last_move_at'])\n",
        "#games_time_dropped = games_positive_rtg.drop('last_move_at', axis=1)\n",
        "\n",
        "games_unique = games_time_dropped.drop_duplicates(keep='first').reset_index(drop=True)\n",
        "print(f\"Original rows: {len(games_time_dropped)}, After removing duplicates: {len(games_unique)}\")"
      ],
      "metadata": {
        "id": "oLMrIWRL0QJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All duplicates removed"
      ],
      "metadata": {
        "id": "DPmcgL1z1-3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert increment codes"
      ],
      "metadata": {
        "id": "iPzDDodQR9CJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll try to handle the increment code in two ways:\n",
        "\n",
        "\n",
        "1.   Separate time into minutes and time increment per move\n",
        "2.   Classify each increment code into a time control\n",
        "\n"
      ],
      "metadata": {
        "id": "IXTlPx3LUImw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert into initial time and increment move"
      ],
      "metadata": {
        "id": "eqnXu79ySX-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "increment_code = games_unique['increment_code']\n",
        "increment_code_split = [time.split('+') for time in increment_code]\n",
        "print('Splitted increment codes:', increment_code_split)\n",
        "\n",
        "#As minutes and increment\n",
        "start_time = [int(minutes[0]) for minutes in increment_code_split]\n",
        "print('Starting time in minutes:', start_time)\n",
        "\n",
        "#bar_chart(list(games_unique.iloc()), start_time, 'Starting time per game ID')\n",
        "\n",
        "increment = [int(seconds[1]) for seconds in increment_code_split]\n",
        "print('Increment in seconds:', increment)\n",
        "\n"
      ],
      "metadata": {
        "id": "NMOMe-v5P5y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_distribution(start_time, 'hist', 'Starting time distribution', 'Minutes', show_stats=True)"
      ],
      "metadata": {
        "id": "ekjG5XXoMg_u",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_distribution(increment, 'hist', 'Increment distribution' 'Seconds', show_stats=True)"
      ],
      "metadata": {
        "id": "xBDHJwavMn5I",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time_df = pd.DataFrame(start_time, columns=['start_time'])\n",
        "increment_df = pd.DataFrame(increment, columns=['increment'])\n",
        "#check if there are games with 0 < start time < 1\n",
        "#games_unique\n",
        "under_minute = ((start_time_df < 1) & (start_time_df > 0)).sum()\n",
        "print('Games with less than 1 minute of start time:', under_minute.iloc[0])\n"
      ],
      "metadata": {
        "id": "PTdeHlQuOpJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most games are finish (no increment) and have 10 minutes as start time, with no game starting with less than a minute"
      ],
      "metadata": {
        "id": "RaBIu2LLM9D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "games_unique = pd.concat([games_unique, start_time_df], axis = 1)\n",
        "games_unique = pd.concat([games_unique, increment_df], axis = 1)\n",
        "games_unique.info()"
      ],
      "metadata": {
        "id": "lEQ3Lu1eSiUs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_winner_by(games_unique, 'start_time', title=\"Result of games by start time\", ylabel=\"Start time (minutes)\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aRkFW_dxWMPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_winner_by(games_unique, 'increment', title=\"Result of games by increment\", ylabel=\"Increment (seconds)\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pMIQjcH0WNwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert into increment codes"
      ],
      "metadata": {
        "id": "J57HIoVwS2Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the data source, [Lichess](https://lichess.org/faq#time-controls), time controls are decided assuming a game length of 40 moves and assigning the following categories depending on the duration:\n",
        "\n",
        "    ≤ 29s = UltraBullet\n",
        "    ≤ 179s = Bullet\n",
        "    ≤ 479s = Blitz\n",
        "    ≤ 1499s = Rapid\n",
        "    ≥ 1500s = Classical"
      ],
      "metadata": {
        "id": "X437Tun_NLgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_time_control(minutes, increment):\n",
        "    total_time = minutes*60+increment*40\n",
        "    if total_time <= 29:\n",
        "        return 'UltraBullet'\n",
        "    elif total_time <= 179:\n",
        "        return 'Bullet'\n",
        "    elif total_time <= 479:\n",
        "        return 'Blitz'\n",
        "    elif total_time <= 1499:\n",
        "        return 'Rapid'\n",
        "    else:\n",
        "        return 'Classical'\n",
        "\n",
        "time_control = games_unique.apply(lambda x: set_time_control(x['start_time'], x['increment']), axis=1)\n",
        "time_control_df = pd.DataFrame({'time_control': time_control})\n",
        "time_control_df.info()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sJR_FiBcNK04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_distribution(time_control, \"count\",'Time control count', 'Time control')"
      ],
      "metadata": {
        "id": "v4kGkBEuet23",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Too few blitz games to the point they are not even appreciated"
      ],
      "metadata": {
        "id": "yc1zgMW9hal-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blitz = [i for i in time_control if i == 'Blitz']\n",
        "print('Number of blitz games:', len(blitz))"
      ],
      "metadata": {
        "id": "vhXElTvMg_8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very few blitz games\n",
        "Since there are too few it would cause troubles during the training and testing, so I dont consider this approach to be viable any longer and will not proceed with it"
      ],
      "metadata": {
        "id": "zNAR7PWthiU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of drawed games"
      ],
      "metadata": {
        "id": "LpP3reHmU-IZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accordint to the stats report, not all games with a winner status of draw have a winner value of draw"
      ],
      "metadata": {
        "id": "xqTUGAch1sN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "draws = games_unique[['victory_status','winner']]\n",
        "draws = draws[games_unique['winner'] == 'draw']\n",
        "not_draw = draws[draws['victory_status'] != 'draw']\n",
        "plot_distribution(draws['victory_status'], 'count', 'Victory status of draws', 'Victory Status')"
      ],
      "metadata": {
        "id": "A2GlTwgMtjnd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The alternative victory status  of drawed games is out of time, which makes sense since the game can result in a draw by insufficient winning material even when running out of time, so it is not a recording error"
      ],
      "metadata": {
        "id": "l9i-7ydE2GCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rating difference"
      ],
      "metadata": {
        "id": "fi_PrES9XUyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will also add the rating difference as a feature to see if it is useful for the predictions"
      ],
      "metadata": {
        "id": "f2pVSNyGMwAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "games_unique['rating_diff'] = games_unique['white_rating'] - games_unique['black_rating']\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Violin plot with boxplot and points\n",
        "sns.violinplot(y=games_unique['rating_diff'], inner=None, color=\"lightblue\")  # violin\n",
        "sns.boxplot(y=games_unique['rating_diff'], width=0.1, color=\"white\", fliersize=0)\n",
        "\n",
        "plt.title(\"Rating difference violing plot\")\n",
        "plt.ylabel(\"Rating difference\")\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "d0mAZ2cxNChX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(games_unique['rating_diff'], kde=True, bins=50)\n",
        "plt.xlabel(\"Rating difference (White - Black)\")\n",
        "plt.title(\"Distribution of rating differences\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zCVs8dKxXAx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical rating difference\n",
        "plot_winner_by(games_unique, 'rating_diff', title=\"Result of games by higher rating player in rating\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7H_O5dBIbsWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Game Evaluation"
      ],
      "metadata": {
        "id": "iIB6Nh04VgdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally,as a simple approach to implement game information I am getting the final evaluation of the position from the algebraic notation of the game\n",
        "\n",
        "This is a lengthy process, so an excel file with the processed results is provided while the cell is commented"
      ],
      "metadata": {
        "id": "RQRgvnp8YJRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removal of short games"
      ],
      "metadata": {
        "id": "SKGS3nxd7DqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid having a short game in which one of the players made a sudden abandonment or ran out of time I will filter all the games that did not reach left the opening phase and not entered the middle game\n",
        "\n",
        "While there is no clear division between the end of the opening and the start of the middle game, I will assume a that middlegame starts after move 15 (30 turns) and remove games shorter than that"
      ],
      "metadata": {
        "id": "gSvzIFjj7Hgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_distribution(games_unique['turns'], 'hist', 'Number of moves per game', 'Number of moves', bins=50)"
      ],
      "metadata": {
        "id": "N7bCUkg2t03V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only rows with at least 30 moves\n",
        "games_long = games_unique.copy()\n",
        "games_long = games_long[games_long['moves'].str.split().str.len() >= 30]\n",
        "\n",
        "games_long = games_long.reset_index(drop=True)\n",
        "games_long.info()"
      ],
      "metadata": {
        "id": "c2oSuwxW8Q3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Game evaluation"
      ],
      "metadata": {
        "id": "zHR5vAbO9f2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "engine = chess.engine.SimpleEngine.popen_uci(\"/usr/games/stockfish\")\n",
        "def final_eval(moves, idx=None, depth=15):\n",
        "    \"\"\"\n",
        "    Evaluate final position from White's perspective (positive = White better)\n",
        "    as centipawn evaluation. Handles notation cleanup, invalid moves,\n",
        "    and malformed strings.\n",
        "    \"\"\"\n",
        "    if idx is not None and idx % 100 == 0:\n",
        "        print(f\"{idx} rows analyzed\")\n",
        "\n",
        "    # Clean move text: remove numbers, results, and dots\n",
        "    moves = re.sub(r\"\\d+\\.\", \"\", moves)\n",
        "    moves = re.sub(r\"(1-0|0-1|1/2-1/2|\\*)\", \"\", moves)\n",
        "    moves = moves.strip()\n",
        "\n",
        "    board = chess.Board()\n",
        "\n",
        "    # Try to play all moves\n",
        "    for move in moves.split():\n",
        "        try:\n",
        "            board.push_san(move)\n",
        "        except Exception:\n",
        "            # If illegal move return determined value\n",
        "            return 23.04\n",
        "\n",
        "    # Run engine analysis\n",
        "    info = engine.analyse(board, chess.engine.Limit(depth=depth))\n",
        "    score = info[\"score\"].pov(chess.WHITE).score(mate_score=20000)\n",
        "    return score\n",
        "\n",
        "\n",
        "games_processed = games_long.copy()\n",
        "games_processed[\"final_eval\"] = [\n",
        "    final_eval(moves, idx=i) for i, moves in enumerate(games_processed[\"moves\"])\n",
        "]\n",
        "engine.quit()\n",
        "games_processed.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_3ayMPp5unUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from google.colab import files\n",
        "\n",
        "# Suppose your DataFrame is called df\n",
        "games_processed.to_csv(\"games_processed.csv\", index=False)  # Save to CSV file\n",
        "\n",
        "# Download to your computer\n",
        "files.download(\"games_processed.csv\")\n",
        "'''"
      ],
      "metadata": {
        "id": "B0nqzlZTFpiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "url_processed = 'https://docs.google.com/uc?export=download&id=1mLUJ0b4z28_y8vngISKJxjhwIlbBwm9j'\n",
        "output_processed = path + '/games_processed.csv'\n",
        "!wget -O \"{output}\" \"{url_processed}\"\n",
        "\n",
        "games_processed = pd.read_csv(output_processed, sep=',')\n",
        "games_processed.head()\n",
        "'''"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IOCouw_FZpN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_distribution(games_processed['final_eval'], 'hist', 'Engine Evaluation distribution', 'Engine Evaluation', bins = 60, log =True)"
      ],
      "metadata": {
        "id": "7KoExYVOzxaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.violinplot(\n",
        "    x='winner',\n",
        "    y='final_eval',\n",
        "    data=games_processed,\n",
        "    inner='quartile',\n",
        "    palette='colorblind'\n",
        ")\n",
        "plt.yscale('symlog', linthresh=100)\n",
        "plt.xlabel('Game Result')\n",
        "plt.ylabel('Engine Evaluation')\n",
        "plt.title('Distribution of Engine Evaluations by Game Result')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pyWekfDInhT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeat exploratory analysis"
      ],
      "metadata": {
        "id": "kpgKol5nD90B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(games_processed,title=\"Games after processing report\")\n",
        "\n",
        "profile.to_file(\"games_processed_report.html\")\n",
        "# show an HTML file inside the notebook\n",
        "HTML(filename=\"games_processed_report.html\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QptKO72_f6Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling"
      ],
      "metadata": {
        "id": "VY-mXcqqVQcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data split"
      ],
      "metadata": {
        "id": "9Vhq4j41YDsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop unused features"
      ],
      "metadata": {
        "id": "Kwk7OTzRZeYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "games_selected = games_processed.drop(['id',\n",
        "                                        'turns',\n",
        "                                        'increment_code',\n",
        "                                        'victory_status',\n",
        "                                        'white_id',\n",
        "                                        'black_id',\n",
        "                                        'moves',\n",
        "                                        'opening_eco',\n",
        "                                        'opening_name',\n",
        "                                        'opening_ply',], axis = 1)"
      ],
      "metadata": {
        "id": "jfdT4zSaZgn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def define_train_test(df):\n",
        "    #Defines predictors and class variable and returns the train and test datasets\n",
        "    #If submission = True, it returns X and y without splitting since it will be used for training new dataset\n",
        "\n",
        "    target = 'winner'\n",
        "    y = df[target]\n",
        "    X = df.drop(target, axis = 1)\n",
        "\n",
        "    #80/20 split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state = 42)\n",
        "\n",
        "    print('Shape of Data (20%)')\n",
        "    print(\"X_train shape : \", X_train.shape)\n",
        "    print(\"y_train shape : \", y_train.shape)\n",
        "    print(\"X_test shape : \", X_test.shape)\n",
        "    print(\"y_test shape : \", y_test.shape)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = define_train_test(games_selected)"
      ],
      "metadata": {
        "id": "FobLBjTK89JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_selected.info()\n",
        "X_train.info()"
      ],
      "metadata": {
        "id": "tpYlbfSUA8qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "G50-CgipYLWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def encode (y_tra, y_te):\n",
        "    encoder = LabelEncoder()\n",
        "    y_tra = encoder.fit_transform(y_tra)\n",
        "    y_te = encoder.transform(y_te)\n",
        "    return y_tra, y_te, encoder\n",
        "\n",
        "y_train, y_test, le = encode(y_train, y_test)"
      ],
      "metadata": {
        "id": "U0YpZyHuZvxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model optimization"
      ],
      "metadata": {
        "id": "oZVa_aJ-YOyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "def optimize_model(X_tr, y_tr):\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "    def logging_callback(study, frozen_trial):\n",
        "        previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n",
        "        if previous_best_value != study.best_value:\n",
        "            study.set_user_attr(\"previous_best_value\", study.best_value)\n",
        "            print(\n",
        "                \"Trial {} finished with best value: {} and parameters: {}. \".format(\n",
        "                frozen_trial.number,\n",
        "                frozen_trial.value,\n",
        "                frozen_trial.params,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def objective(trial, X, y):\n",
        "\n",
        "        def rf_model(trial):\n",
        "            #Objective function for bayesian hyperparameter optimization of a\n",
        "            #Random Forest classifier using Optuna\n",
        "            params = {\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 250),\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
        "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 10, 30),\n",
        "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\",  5, 20),\n",
        "                \"max_features\": trial.suggest_int(\"max_features\", 1,5),\n",
        "                \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
        "                \"random_state\": 42\n",
        "\n",
        "            }\n",
        "            model = RandomForestClassifier(**params, n_jobs=-1)\n",
        "            return model\n",
        "\n",
        "        def lgbm_model(trial):\n",
        "            #Objective function for bayesian hyperparameter optimization of a\n",
        "            #Light Gradient-Boosting Machine using Optuna\n",
        "            params = {\n",
        "                \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1000),\n",
        "                \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1000),\n",
        "                \"random_state\": 42,\n",
        "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "            }\n",
        "            model = LGBMClassifier(**params, verbose=-1)\n",
        "            return model\n",
        "\n",
        "        def xgb_model(trial):\n",
        "            #Objective function for bayesian hyperparameter optimization of an\n",
        "            #eXtreme Gradient Boostin classifier using Optuna\n",
        "            params = {\n",
        "                \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 1000),\n",
        "                \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 1000),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "                \"eval_metric\": \"mlogloss\",\n",
        "                \"random_state\": 42\n",
        "            }\n",
        "            model = XGBClassifier(**params)\n",
        "            return model\n",
        "\n",
        "        # Select which model to use\n",
        "        model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"XGBoost\", \"Light Gradient-Boosting Machine\"])\n",
        "\n",
        "        if model_name == \"RandomForest\":\n",
        "            model = rf_model(trial)\n",
        "        elif model_name == \"XGBoost\":\n",
        "            model = xgb_model(trial)\n",
        "        else:\n",
        "            model = lgbm_model(trial)\n",
        "\n",
        "        # Evaluate with stratified cross-validation\n",
        "        stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        recall_scorer = make_scorer(recall_score, average='macro')\n",
        "        score = cross_val_score(model, X, y, cv=stratified_cv, scoring=recall_scorer).mean()\n",
        "        return score\n",
        "\n",
        "\n",
        "    #Create optuna study\n",
        "    sampler = TPESampler(seed=10)\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler = sampler)\n",
        "    #Optimize study\n",
        "    study.optimize(lambda trial: objective(trial, X_tr, y_tr), n_trials=200, callbacks = [logging_callback])\n",
        "    print(f\"Best averaged recall: {study.best_value:.4f}\")\n",
        "    best_params = study.best_params.copy()\n",
        "    print(f\"Best hyperparameters: {best_params}\")\n",
        "    return best_params\n",
        "\n",
        "optimized = optimize_model(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "ytusC_WiVCeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting and predictions"
      ],
      "metadata": {
        "id": "MN-5CcVC20Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_predict(params, X_tr, X_te, y_tr, y_te):\n",
        "    best_params = params.copy()\n",
        "    best_model_name = best_params.pop(\"model\")\n",
        "\n",
        "    if best_model_name == \"RandomForest\":\n",
        "        best_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)\n",
        "    elif best_model_name == \"XGBoost\":\n",
        "        best_model = XGBClassifier(**best_params, eval_metric=\"logloss\", random_state=42)\n",
        "    else:  # Light Gradient-Boosting Machine\n",
        "        best_model = LGBMClassifier(**best_params, max_iter=1000, random_state=42)\n",
        "\n",
        "    best_model.fit(X_tr, y_tr)\n",
        "    test_recall = best_model.score(X_te, y_te)\n",
        "    print(f\"Best model: {best_model_name}\")\n",
        "    print(f\"Test set recall: {test_recall:.4f}\")\n",
        "    y_predictions = best_model.predict(X_te)\n",
        "    y_probabilities = best_model.predict_proba(X_te)\n",
        "    return best_model, y_predictions, y_probabilities\n",
        "\n",
        "fitted_model, y_pred, y_pred_proba = model_predict(optimized, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "kY6QTni4YiiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results"
      ],
      "metadata": {
        "id": "owsT_PP4Yewt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importance"
      ],
      "metadata": {
        "id": "pHqGbys_YgeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_importance(best_model, X):\n",
        "    # Calculate feature importance\n",
        "\n",
        "    importances = best_model.feature_importances_\n",
        "    features = X.columns\n",
        "\n",
        "    #Normalize importance\n",
        "    importances = importances / importances.sum()\n",
        "\n",
        "    # Put into a DataFrame for easy sorting\n",
        "    feat_imp = pd.DataFrame({\n",
        "        \"Feature\": features,\n",
        "        \"Importance\": importances\n",
        "    }).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.barh(feat_imp[\"Feature\"], feat_imp[\"Importance\"], color=\"skyblue\")\n",
        "    plt.gca().invert_yaxis()  # Most important at the top\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.title(\"Feature Importance\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "feature_importance(fitted_model, X_train)"
      ],
      "metadata": {
        "id": "vR2WOdE6xwuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification metrics"
      ],
      "metadata": {
        "id": "aUaFButlYkb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def classification_metrics(best_model, X, y, encoder):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix and print classification metrics.\n",
        "    \"\"\"\n",
        "    # Predict encoded labels\n",
        "    y_pred = best_model.predict(X)\n",
        "\n",
        "    # Decode both true and predicted labels\n",
        "    y_true_decoded = encoder.inverse_transform(y)\n",
        "    y_pred_decoded = encoder.inverse_transform(y_pred)\n",
        "\n",
        "    labels = ['white', 'draw', 'black']\n",
        "\n",
        "    # Confusion matrix (use decoded labels)\n",
        "    cm = confusion_matrix(y_true_decoded, y_pred_decoded, labels=labels, normalize='true')\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    disp.plot(cmap='Blues', ax=ax, colorbar=False)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Classification report (also use decoded labels)\n",
        "    print(\"\\nClassification report:\\n\")\n",
        "    print(classification_report(y_true_decoded, y_pred_decoded, target_names=labels))\n",
        "\n",
        "\n",
        "\n",
        "classification_metrics(fitted_model, X_test, y_test, le)\n"
      ],
      "metadata": {
        "id": "ABI4i_oapPz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC curve"
      ],
      "metadata": {
        "id": "UPTsoMltYu0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "\n",
        "def plot_multiclass_roc(y_true, y_proba, model, label_encoder):\n",
        "    \"\"\"\n",
        "    Plots ROC curves for multi-class classification.\n",
        "    y_true should be integer-encoded.\n",
        "    y_pred_proba should be probability outputs with shape (n_samples, n_classes).\n",
        "    \"\"\"\n",
        "    # Use the encoder’s class order to ensure consistency\n",
        "    classes = label_encoder.classes_\n",
        "    n_classes = len(classes)\n",
        "\n",
        "    # Binarize the true labels according to the class order\n",
        "    y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
        "\n",
        "    # Colorblind-friendly palette\n",
        "    colors = sns.color_palette(\"colorblind\", n_classes)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Per-class ROC curves\n",
        "    for i, cls in enumerate(classes):\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_proba[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, lw=2, color=colors[i], label=f'{cls} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    # Macro-average ROC\n",
        "    all_fpr = np.unique(\n",
        "        np.concatenate([roc_curve(y_true_bin[:, i], y_proba[:, i])[0] for i in range(n_classes)])\n",
        "    )\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_proba[:, i])\n",
        "        mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
        "    mean_tpr /= n_classes\n",
        "    roc_auc_macro = auc(all_fpr, mean_tpr)\n",
        "    plt.plot(all_fpr, mean_tpr, color='navy', linestyle='-',\n",
        "             label=f'Macro-average ROC (AUC = {roc_auc_macro:.2f})', lw=2)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Multi-class ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_multiclass_roc(y_test, y_pred_proba, fitted_model, le)\n"
      ],
      "metadata": {
        "id": "JCa9NavCdaRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision Recall curve"
      ],
      "metadata": {
        "id": "Ae1uGFdEDgY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "def plot_precision_recall_curve(y_true, y_proba, model, label_encoder):\n",
        "    \"\"\"\n",
        "    Plots precision–recall curves for multi-class classification.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: encoded (numeric) labels\n",
        "    - y_prob: predicted probabilities (n_samples × n_classes)\n",
        "    - model: fitted classifier\n",
        "    - label_encoder: fitted LabelEncoder\n",
        "    \"\"\"\n",
        "    classes = label_encoder.classes_\n",
        "    n_classes = len(classes)\n",
        "\n",
        "    # Binarize true labels using the encoder’s order\n",
        "    y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
        "\n",
        "    # Colorblind friendly palette\n",
        "    colors = sns.color_palette(\"colorblind\", n_classes)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Per-class PR curves\n",
        "    for i, cls in enumerate(classes):\n",
        "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_proba[:, i])\n",
        "        ap = average_precision_score(y_true_bin[:, i], y_proba[:, i])\n",
        "        plt.plot(\n",
        "            recall, precision, lw=2, color=colors[i],\n",
        "            label=f'{cls} (AP = {ap:.2f})'\n",
        "        )\n",
        "\n",
        "    # Macro-average curve\n",
        "    all_recall = np.unique(np.concatenate([\n",
        "        precision_recall_curve(y_true_bin[:, i], y_proba[:, i])[1] for i in range(n_classes)\n",
        "    ]))\n",
        "    mean_precision = np.zeros_like(all_recall)\n",
        "    for i in range(n_classes):\n",
        "        p, r, _ = precision_recall_curve(y_true_bin[:, i], y_proba[:, i])\n",
        "        mean_precision += np.interp(all_recall, r[::-1], p[::-1])\n",
        "    mean_precision /= n_classes\n",
        "    mean_ap = np.mean([\n",
        "        average_precision_score(y_true_bin[:, i], y_proba[:, i]) for i in range(n_classes)\n",
        "    ])\n",
        "    plt.plot(\n",
        "        all_recall, mean_precision,\n",
        "        color='black', linestyle='--', lw=2,\n",
        "        label=f'Macro-average (AP = {mean_ap:.2f})'\n",
        "    )\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Multi-class Precision–Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "plot_precision_recall_curve(y_test, y_pred_proba, fitted_model, le)"
      ],
      "metadata": {
        "id": "6Mmh9pmBpVUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second model"
      ],
      "metadata": {
        "id": "r5pncpcpz9qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to retry it by removing redundant features from the input, so I will make a pipeline"
      ],
      "metadata": {
        "id": "rigwaozAy0Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Suppose your DataFrame is called df\n",
        "games_processed.to_csv(\"games_processed.csv\", index=False)  # Save to CSV file\n",
        "\n",
        "# Download to your computer\n",
        "files.download(\"games_processed.csv\")"
      ],
      "metadata": {
        "id": "-jPiDTbH642W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "games_selected_v2 = games_selected.drop(['avg_rating',\n",
        "                                         'rated',\n",
        "                                         'increment',], axis = 1)"
      ],
      "metadata": {
        "id": "FTL9T9RBy61d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(df):\n",
        "    X_train_v2, X_test_v2, y_train_v2, y_test_v2 = define_train_test(games_selected_v2)\n",
        "    y_train_v2, y_test_v2, le_v2 = encode(y_train_v2, y_test_v2)\n",
        "    optimized_v2 = optimize_model(X_train_v2, y_train_v2)\n",
        "    fitted_model_v2, y_pred_v2, y_pred_proba_v2 = model_predict(optimized_v2, X_train_v2, X_test_v2, y_train_v2, y_test_v2)\n",
        "    classification_metrics(fitted_model_v2, X_test_v2, y_test_v2, le_v2)\n",
        "    plot_multiclass_roc(y_test_v2, y_pred_proba_v2, fitted_model_v2, le_v2)\n",
        "    plot_precision_recall_curve(y_test_v2, y_pred_proba_v2, fitted_model_v2, le_v2)\n",
        "\n",
        "    return fitted_model_v2, X_train_v2, X_test_v2, y_train_v2, y_test_v2, y_pred_v2, y_pred_proba_v2, le_v2\n",
        "\n",
        "fitted_model_v2, X_train_v2, X_test_v2, y_train_v2, y_test_v2, y_pred_v2, y_pred_proba_v2, le_v2 = run_model(games_selected_v2)"
      ],
      "metadata": {
        "id": "lET3XOGO0EUf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}